{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu1CcMlRlhtO"
      },
      "source": [
        "# Sequential single-point Bayesian optimisation\n",
        "\n",
        "In this example, NUBO is used for sequential single-point optimisation. The `Hartmann6D` synthetic test function acts as a surrogate for a black box objective funtion, such as an experiment or a simulation. We use the analytical acquisiton function `UpperConfidenceBound` with $\\beta = 1.96^2$ corresponding to the 95% confidence interval of the Gaussian distribution. We optimise this acquisition function with the `lbfgsb()` algorithm with 5 starts to avoid getting stuck in a local maximum. The optimisation loop is run for 40 iterations and finds a solution close the true optimum of -3.3224."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ivtHr7RbldhU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best at evaluation 33: \t Inputs: [0.1247 0.     0.634  0.2347 0.257  0.4351], \t Outputs: [-1.7711]\n",
            "New best at evaluation 37: \t Inputs: [0.1196 0.0298 0.2002 0.2853 0.3047 0.5265], \t Outputs: [-2.3202]\n",
            "New best at evaluation 38: \t Inputs: [0.1407 0.     0.2125 0.2922 0.3278 0.6806], \t Outputs: [-2.6529]\n",
            "New best at evaluation 40: \t Inputs: [0.1356 0.     0.3436 0.3205 0.3754 0.6734], \t Outputs: [-2.6605]\n",
            "New best at evaluation 44: \t Inputs: [0.1756 0.     0.351  0.3389 0.3135 0.637 ], \t Outputs: [-2.8567]\n",
            "New best at evaluation 45: \t Inputs: [0.1758 0.     0.4007 0.2858 0.2995 0.6475], \t Outputs: [-3.0165]\n",
            "New best at evaluation 46: \t Inputs: [0.1803 0.1282 0.4165 0.2719 0.299  0.6528], \t Outputs: [-3.2702]\n",
            "New best at evaluation 50: \t Inputs: [0.205  0.1713 0.4946 0.2829 0.3034 0.6497], \t Outputs: [-3.3072]\n",
            "New best at evaluation 54: \t Inputs: [0.2177 0.1405 0.474  0.2793 0.2996 0.6665], \t Outputs: [-3.3091]\n",
            "New best at evaluation 55: \t Inputs: [0.2128 0.1415 0.4793 0.2709 0.3187 0.6467], \t Outputs: [-3.314]\n",
            "New best at evaluation 56: \t Inputs: [0.1959 0.1521 0.4704 0.281  0.3138 0.6611], \t Outputs: [-3.3199]\n",
            "New best at evaluation 58: \t Inputs: [0.2003 0.1573 0.4749 0.2769 0.3109 0.6575], \t Outputs: [-3.3216]\n",
            "Evaluation: 58 \t Solution: -3.3216\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from nubo.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
        "from nubo.models import GaussianProcess, fit_gp\n",
        "from nubo.optimisation import lbfgsb\n",
        "from nubo.test_functions import Hartmann6D\n",
        "from nubo.utils import gen_inputs\n",
        "from gpytorch.likelihoods import GaussianLikelihood\n",
        "\n",
        "\n",
        "# test function\n",
        "func = Hartmann6D(minimise=False)\n",
        "dims = func.dims\n",
        "bounds = func.bounds\n",
        "\n",
        "# training data\n",
        "x_train = gen_inputs(num_points=dims*5,\n",
        "                     num_dims=dims,\n",
        "                     bounds=bounds)\n",
        "y_train = func(x_train)\n",
        "\n",
        "# Bayesian optimisation loop\n",
        "iters = 40\n",
        "\n",
        "for iter in range(iters):\n",
        "    \n",
        "    # specify Gaussian process\n",
        "    likelihood = GaussianLikelihood()\n",
        "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
        "    \n",
        "    # fit Gaussian process\n",
        "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
        "\n",
        "    # specify acquisition function\n",
        "    # acq = ExpectedImprovement(gp=gp, y_best=torch.max(y_train))\n",
        "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
        "\n",
        "    # optimise acquisition function\n",
        "    x_new, _ = lbfgsb(func=acq, bounds=bounds, num_starts=5)\n",
        "\n",
        "    # evaluate new point\n",
        "    y_new = func(x_new)\n",
        "    \n",
        "    # add to data\n",
        "    x_train = torch.vstack((x_train, x_new))\n",
        "    y_train = torch.hstack((y_train, y_new))\n",
        "\n",
        "    # print new best\n",
        "    if y_new > torch.max(y_train[:-1]):\n",
        "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
        "\n",
        "# results\n",
        "best_iter = int(torch.argmax(y_train))\n",
        "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
