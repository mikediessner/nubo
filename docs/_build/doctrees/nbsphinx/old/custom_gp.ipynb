{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Gaussian process\n",
    "\n",
    "This notebook gives a introduction to specifying custom Gaussian process with ``GPyTorch`` that can be used with NUBO.\n",
    "\n",
    "## Define Gaussian process\n",
    "\n",
    "A Gaussian process is defined by its mean function and its covariance kernel. Both are specified in the ``__init__()`` method of the the ``GaussianProcess`` class below and can be easily swapped out by the desired function and kernel. While ``GPyTorch`` offers many different options, the most common choices are the zero mean or constant mean function and the Matern or RBF kernel. Some kernel, such as the Matern and the RBF kernel, are only defined for a certain range so that they need to be scaled through the ``ScaleKernel`` to be used with all problems. The length-scale parameters of the covariance kernel can either be represented as a single length-scale or as one length-scale parameter for each input dimensions. The latter is known as automatic relevance determination (ARD) and allows inputs to be differently correlated. The ``forward()`` method takes a test point and returns the predictive multivariate normal distribution. All other properties of the Gaussian process are inherited by the ``ExactGP`` making it easy to implement custom Gaussian processes. For more information about Gaussian processes and about options for mean function and covariance kernel see the documentation of ``GPyTorch``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.means import ZeroMean, ConstantMean\n",
    "from gpytorch.kernels import MaternKernel, RBFKernel, ScaleKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import Likelihood\n",
    "\n",
    "\n",
    "class GaussianProcess(ExactGP):\n",
    "\n",
    "    def __init__(self,\n",
    "                 x_train: Tensor, \n",
    "                 y_train: Tensor,\n",
    "                 likelihood: Likelihood) -> None:\n",
    "\n",
    "        # initialise ExactGP\n",
    "        super(GaussianProcess, self).__init__(x_train, y_train, likelihood)\n",
    "\n",
    "        # specify mean function and covariance kernel\n",
    "        self.mean_module = ZeroMean()\n",
    "        self.covar_module = ScaleKernel(\n",
    "            base_kernel=RBFKernel(ard_num_dims=x_train.shape[-1])\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormal:\n",
    "\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "\n",
    "        return MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data\n",
    "\n",
    "To use the Gaussian process, we first generate some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = func.dims\n",
    "bounds = func.bounds\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Gaussian process\n",
    "\n",
    "Before we fit the Gaussian process to the training data, we first have to decide on the likelihood that should be used. There are two likelihoods we want to consider here: First, we have the standard Gaussian likelihood. This likelihood assumes a constant homoskedastic observation noise and estimates the noise parameter $\\sigma^2$ from the data. Second, there is the Gaussian likelihood with fixed noise. You want to use this option when you know or can measure the observation noise of your objective function. In this case, you can still decide if you want to estimate the additional noise besides the observations noise or not. This example continues with the full estimation of the noise level. NUBO has the convenience function ``fit_gp`` that maximises the log marginal likelihood with maximum likelihood estimation (MLE) using ``torch``'s Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nubo.models import fit_gp\n",
    "from gpytorch.likelihoods import GaussianLikelihood, FixedNoiseGaussianLikelihood\n",
    "\n",
    "  \n",
    "# initialise Gaussian process\n",
    "likelihood = GaussianLikelihood()\n",
    "gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "\n",
    "# fit Gaussian process\n",
    "fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions for test points\n",
    "\n",
    "With the fitted Gaussian process in hand, we can easily predict the mean and the variance of previously unobserved test points. Below, we sample five points randomly and print the predictive mean and variance that define the predictive distribution for each test point based on the training data and our Gaussian process specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([ 0.2188,  0.1616, -0.0127,  0.0252, -0.0069], dtype=torch.float64)\n",
      "Variance: tensor([0.0136, 0.0191, 0.0252, 0.0164, 0.0343], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# sample test point\n",
    "x_test = torch.rand((5, dims))\n",
    "\n",
    "# set Gaussian Process to eval mode\n",
    "gp.eval()\n",
    "\n",
    "# make predictions\n",
    "pred = gp(x_test)\n",
    "\n",
    "# predictive mean and variance\n",
    "mean = pred.mean\n",
    "variance = pred.variance.clamp_min(1e-10)\n",
    "\n",
    "print(f\"Mean: {mean.detach()}\")\n",
    "print(f\"Variance: {variance.detach()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
