{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical problems\n",
    "\n",
    "This notebook covers some problems that are commonly encountered in black-box optimisation and how they can be optimised with the off-the-shelf `optimise` function. This function combines everything required for one optimisation step and returns one or multiple candidate points.\n",
    "\n",
    "## Single-point optimisation\n",
    "\n",
    "In this example, NUBO is used for sequential single-point optimisation. The `Hartmann6D` synthetic test function acts as a substitute for a black-box objective function, such as an experiment or a simulation. The `optimise` function uses the analytical `ExpectedImprovement` acquisition function and optimies it via the L-BFGS-B algorithm by default. The optimisation loop is run for 40 iterations and finds a solution close to the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 31: \t Inputs: [0.4805 0.1888 0.6875 0.1961 0.2543 0.5498], \t Outputs: [-1.761]\n",
      "New best at evaluation 40: \t Inputs: [0.363  0.1418 0.6915 0.2731 0.2711 0.6849], \t Outputs: [-2.5578]\n",
      "New best at evaluation 43: \t Inputs: [0.2742 0.1642 0.496  0.2955 0.2636 0.7238], \t Outputs: [-3.0372]\n",
      "Evaluation: 43 \t Solution: -3.0372\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.algorithms import optimise\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "\n",
    "    # NUBO\n",
    "    x_new = optimise(x_train, y_train, bounds=bounds)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "\n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained multi-point optimisation\n",
    "\n",
    "In this example, NUBO is used to perform multi-point optimisation that allows the candidates to be evaluated from the objective function in parallel. Multi-point optimisation is implemented in NUBO through Monte Carlo acquisition functions. The script below uses the `MCUpperConfidenceBound` acquisition function with 64 samples. Each batch of 4 is found sequentially (also known as greedy optimisation) by optimising the acquisition function usually with the stochastic Adam optimiser. However, we also consider two constraints on the input space to showcase the two different options: equality constraints and inequality constraints. Equality constraints require the constraint to be 0 while the result is non-negative for inequality constraints. Our first constraint {'type': 'ineq', 'fun': lambda x: 0.5 - x[0] - x[1]} is an inequality constraint and requires the sum of the first two inputs to be smaller or equal to 0.5. The second constraint {'type': 'eq', 'fun': lambda x: 1.2442 - x[3] - x[4] - x[5]} is an equality constraint specifying that the sum of the last three inputs needs to be equal to 1.2442. The `Hartmann6D` synthetic test function acts as a substitute for a black-box objective function, such as an experiment or a simulation. The optimisation loop is run for 10 iterations returning batches of 4 each (a total of 40 evaluations) and finds a solution close to the true optimum of -3.3224.\n",
    "\n",
    "Important: Generating initial input points with a Latin hypercube might not work for real problems as they will not consider the constraints but only the bounds. In these situations, other methods or selecting initial points by hand might be preferable. The purpose of this example is solely the demonstration of how NUBO handles constraints and constrained optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 35: \t Inputs: [0.3274 0.1726 0.3425 0.2882 0.2615 0.6945], \t Outputs: -2.934\n",
      "New best at evaluation 43: \t Inputs: [0.2317 0.2549 0.3728 0.2457 0.2965 0.7019], \t Outputs: -3.0234\n",
      "New best at evaluation 48: \t Inputs: [0.2224 0.1383 0.4157 0.2715 0.3251 0.6475], \t Outputs: -3.2717\n",
      "New best at evaluation 51: \t Inputs: [0.1882 0.1534 0.4292 0.2797 0.3012 0.6633], \t Outputs: -3.2934\n",
      "New best at evaluation 59: \t Inputs: [0.2108 0.1393 0.5028 0.2754 0.3058 0.6631], \t Outputs: -3.3101\n",
      "New best at evaluation 63: \t Inputs: [0.1893 0.1625 0.4902 0.2713 0.3125 0.6604], \t Outputs: -3.3168\n",
      "Evaluation: 63 \t Solution: -3.3168\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.algorithms import optimise\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "cons = [{'type': 'ineq', 'fun': lambda x: 0.5 - x[0] - x[1]},\n",
    "        {'type': 'eq', 'fun': lambda x: 1.2442 - x[3] - x[4] - x[5]}]\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 10\n",
    "\n",
    "for iter in range(iters):\n",
    "\n",
    "    # NUBO\n",
    "    x_new = optimise(x_train, y_train,\n",
    "                     bounds=bounds,\n",
    "                     batch_size=4,\n",
    "                     acquisition=\"UCB\",\n",
    "                     beta=5.0,\n",
    "                     constraints=cons,\n",
    "                     mc_samples=64)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "\n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if torch.max(y_new) > torch.max(y_train[:-y_new.size(0)]):\n",
    "        best_eval = torch.argmax(y_train)\n",
    "        print(f\"New best at evaluation {best_eval+1}: \\t Inputs: {x_train[best_eval, :].numpy().reshape(dims).round(4)}, \\t Outputs: {-y_train[best_eval].numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy observations with continuous and discrete parameters\n",
    "\n",
    "In this example, NUBO is used for sequential single-point optimisation with continuous and discrete parameters and noisy observations. Additionally to the bounds, a dictionary containing the dimensions as keys and the possible values as values have to be specified for the discrete values. The `Hartmann6D` synthetic test function acts as a substitute for a black-box objective function, such as an experiment or a simulation. We use the analytical acquisiton function `UpperConfidenceBound` by specifying `acquisition=\"UCB\"` with a trade-off parameter `beta=5.0`. The dictionary of discrete values is provided to the `optimise` function and the `noisy` argument is set to `True` to allow he optimisation of a noisy acquisition function. The optimisation loop is run for 40 iterations and finds a solution close to the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 38: \t Inputs: [0.4    1.     0.     0.5369 0.3    0.    ], \t Outputs: [-2.6125]\n",
      "New best at evaluation 42: \t Inputs: [0.4    0.922  0.     0.5468 0.3    0.    ], \t Outputs: [-2.9671]\n",
      "New best at evaluation 45: \t Inputs: [0.4    0.9201 1.     0.5586 0.3    0.    ], \t Outputs: [-3.0494]\n",
      "New best at evaluation 46: \t Inputs: [0.4    0.9158 1.     0.5582 0.3    0.0571], \t Outputs: [-3.1341]\n",
      "New best at evaluation 49: \t Inputs: [0.4    0.8774 1.     0.561  0.3    0.0419], \t Outputs: [-3.1727]\n",
      "New best at evaluation 51: \t Inputs: [0.4    0.8744 1.     0.5736 0.3    0.0454], \t Outputs: [-3.1938]\n",
      "New best at evaluation 52: \t Inputs: [0.4    0.8617 1.     0.5805 0.3    0.0539], \t Outputs: [-3.2136]\n",
      "New best at evaluation 57: \t Inputs: [0.4    0.8721 1.     0.575  0.3    0.0361], \t Outputs: [-3.2376]\n",
      "New best at evaluation 60: \t Inputs: [0.4    0.8715 1.     0.5715 0.3    0.0421], \t Outputs: [-3.2734]\n",
      "Evaluation: 60 \t Solution: -3.2734\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.algorithms import optimise\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False, noise_std=0.05)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "discrete = {0: [0.2, 0.4, 0.6, 0.8], 4: [0.3, 0.6, 0.9]}\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "\n",
    "    # NUBO\n",
    "    x_new = optimise(x_train, y_train,\n",
    "                     bounds=bounds,\n",
    "                     acquisition=\"UCB\",\n",
    "                     beta=5.0,\n",
    "                     discrete=discrete,\n",
    "                     noisy=True)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "\n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
