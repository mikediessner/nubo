{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started\n",
    "\n",
    "This brief introduction will teach you in detail how to install NUBO from the [GitHub repository](https://github.com/mikediessner/nubo) and how to set up a Bayesian optimisation loop to maximise a toy function using NUBO’s predefined Gaussian process as the surrogate model.\n",
    "\n",
    "## Installing NUBO\n",
    "\n",
    "Install NUBO and all its dependencies directly from the [GitHub repository](https://github.com/mikediessner/nubo) using the [Python package manager](https://pip.pypa.io/en/latest/) *pip* with the following code. We recommend the use of a virtual environment.\n",
    "\n",
    "```text\n",
    "pip install git+https://github.com/mikediessner/nubo\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## Optimising a toy function with NUBO\n",
    "\n",
    "The `bounds` of the input space are defined as a two-dimensional `torch.Tensor` where the first row gives the lower bounds for all input dimensions and the second row gives the corresponding upper bounds. In the example above, the bounds are given as an attribute of the `Hartmann6D` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nubo.test_functions import Hartmann6D\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate some initial training data. We decide to generate 5 data points per input dimension resulting in a total of 30 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NUBO, training inputs `x_train` should be a two-dimensional `torch.Tensor` (a matrix), where the rows are individual points and the columns are individual dimensions. In this example, our training data has size 30 x 6. The training outputs `y_train` should be a one-dimensional `torch.Tensor` (a vector) with one entry for each training input (here `y_train` has size 30).\n",
    "\n",
    "Now we can prepare the Bayesian optimisation loop. We choose NUBO’s predefined Gaussian process that by default has a constant mean function and a Matern 5/2 kernel. We also use the Gaussian likelihood to estimate observational noise. We estimate the Gaussian processes hyper-parameters via maximum likelihood estimation (MLE) using the Adam optimiser. For the acquisition function, we implement the analytical upper confidence bound (UCB) with trade-off parameter $\\beta = 1.96^2$ (corresponding to 95% confidence intervals for the Gaussian distribution) and optimise it with the L-BFGS-B algorithm using a multi-start approach with five starts. These multiple starts help to ensure that the optimiser does not get stuck in a local optimum. The Bayesian optimisation loop is run for 40 iterations, giving a total evaluation budget of 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 31: \t Inputs: [0.477  0.0444 0.0736 0.2914 0.3603 0.7323], \t Outputs: [-1.9494]\n",
      "New best at evaluation 34: \t Inputs: [0.4453 0.0418 0.0483 0.3164 0.3478 0.6925], \t Outputs: [-2.0684]\n",
      "New best at evaluation 39: \t Inputs: [0.4127 0.1638 0.     0.277  0.3385 0.679 ], \t Outputs: [-2.1595]\n",
      "New best at evaluation 40: \t Inputs: [0.3715 0.1565 0.     0.3261 0.3372 0.7126], \t Outputs: [-2.1843]\n",
      "New best at evaluation 41: \t Inputs: [0.3589 0.134  0.3895 0.2927 0.3222 0.7003], \t Outputs: [-2.9809]\n",
      "New best at evaluation 42: \t Inputs: [0.2754 0.1478 0.425  0.2529 0.3054 0.6874], \t Outputs: [-3.2027]\n",
      "New best at evaluation 46: \t Inputs: [0.1473 0.1864 0.427  0.2906 0.2993 0.666 ], \t Outputs: [-3.2302]\n",
      "New best at evaluation 51: \t Inputs: [0.1764 0.1303 0.4576 0.3022 0.3029 0.6827], \t Outputs: [-3.2657]\n",
      "New best at evaluation 52: \t Inputs: [0.2016 0.1447 0.4616 0.2798 0.3018 0.6716], \t Outputs: [-3.31]\n",
      "New best at evaluation 53: \t Inputs: [0.2063 0.144  0.465  0.2787 0.3138 0.6519], \t Outputs: [-3.3192]\n",
      "New best at evaluation 58: \t Inputs: [0.205  0.1516 0.4686 0.2725 0.3137 0.6614], \t Outputs: [-3.3206]\n",
      "New best at evaluation 66: \t Inputs: [0.2096 0.142  0.4767 0.2757 0.3112 0.6573], \t Outputs: [-3.3209]\n",
      "New best at evaluation 70: \t Inputs: [0.2076 0.1527 0.4728 0.2802 0.3109 0.6594], \t Outputs: [-3.321]\n"
     ]
    }
   ],
   "source": [
    "from nubo.acquisition import UpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import single\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = single(func=acq, method=\"L-BFGS-B\", bounds=bounds, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the overall best solution: We get -3.3210 on evaluation 70 which approximaties the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: 70 \t Solution: 3.3210\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters of the Gaussian process can be viewed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean function constant: 0.10733954608440399\n",
      "Covariance kernel output-scale: 0.2942888140678406\n",
      "Covariance kernel length-scale: tensor([[0.5552, 0.5305, 0.6730, 0.3610, 0.2741, 0.3786]])\n",
      "Estimated noise/nugget: 0.000116478513518814\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean function constant: {gp.mean_module.constant.item()}\")\n",
    "print(f\"Covariance kernel output-scale: {gp.covar_module.outputscale.item()}\")\n",
    "print(f\"Covariance kernel length-scale: {gp.covar_module.base_kernel.lengthscale.detach()}\")\n",
    "print(f\"Estimated noise/nugget: {likelihood.noise.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
