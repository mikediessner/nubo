<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Citation" href="citation.html" /><link rel="prev" title="Get started" href="get_started.html" />

    <link rel="shortcut icon" href="_static/favicon.ico"/><!-- Generated with Sphinx 6.1.3 and Furo 2022.12.07 -->
        <title>A primer on Bayesian optimisation - NUBO</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">NUBO</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">NUBO</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">NUBO:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Get started</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">A primer on Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation</a></li>
<li class="toctree-l1"><a class="reference external" href="http://github.com/mikediessner/nubo-dev">Github</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="singlepoint.html">Sequential single-point Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="multipoint_joint.html">Parallel multi-point joint Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="multipoint_sequential.html">Parallel multi-point sequential Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="asynchronous_bo.html">Asynchronous Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constrained_bo.html">Constrained Bayesian optimisation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="nubo.acquisition.html">Acquisition module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.models.html">Surrogates module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.optimisation.html">Optimisation module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.test_functions.html">Test function module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.utils.html">Utility module</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="a-primer-on-bayesian-optimisation">
<span id="bo"></span><h1>A primer on Bayesian optimisation<a class="headerlink" href="#a-primer-on-bayesian-optimisation" title="Permalink to this heading">#</a></h1>
<section id="maximisation-problem">
<span id="objfunc"></span><h2>Maximisation problem<a class="headerlink" href="#maximisation-problem" title="Permalink to this heading">#</a></h2>
<p>Bayesian optimisation aims to solve the $d$-dimensional maximisation problem</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\boldsymbol  x^* = \arg \max_{\boldsymbol  x \in \mathcal{X}} f(\boldsymbol  x)\]</div>
</div>
<p>where the input space is usually continuous and bounded by a hyper-rectangle <span class="math notranslate nohighlight">\(\mathcal{X} \in [a, b]^d\)</span> with <span class="math notranslate nohighlight">\(a, b \in \mathbb{R}\)</span>. The function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span> is most commonly a derivative-free expensive-to-evaluate black box problem that only allows inputs <span class="math notranslate nohighlight">\(\boldsymbol x_i\)</span> to be querried and outputs <span class="math notranslate nohighlight">\(y_i\)</span> to be observed without gaining any further insights into the underlying system. We assume any noise <span class="math notranslate nohighlight">\(\epsilon\)</span> that is introduced when taking measurements to be independent and identically distributed Gaussian noise <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N} (0, \sigma^2)\)</span> such that <span class="math notranslate nohighlight">\(y_i = f(\boldsymbol  x_i) + \epsilon\)</span>. Hence, a set of $n$ pairs of data points and corresponding observations is defined as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{D_n} = \{(\boldsymbol x_i, y_i)\}_{i=1}^n\]</div>
</div>
<p>and we further define training inputs as matrix <span class="math notranslate nohighlight">\(\boldsymbol X_n = \{\boldsymbol x_i \}_{i=1}^n\)</span> and their training outputs as vector <span class="math notranslate nohighlight">\(\boldsymbol y_n = \{y_i\}_{i=1}^n\)</span>.</p>
</section>
<section id="bayesian-optimisation">
<h2>Bayesian optimisation<a class="headerlink" href="#bayesian-optimisation" title="Permalink to this heading">#</a></h2>
<p>Bayesian optimisation <a class="footnote-reference brackets" href="#id18" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id20" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id21" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id24" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id25" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> is a surrogate model-based optimisation algorithm that aims to maximise the objective function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span> in a minimum number of function evaluations. Usually, the objective function does not have a known mathematical expression and every function evaluation is expensive requiring a cost-effective and sample-efficient optimisation routine. Bayesian optimisation meets these criteria by representing the objective function through a surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, often a Gaussian process <span class="math notranslate nohighlight">\(\mathcal{GP}\)</span>. This representation can then be used to find the next point that should be evaluated by maximising a criterion specified through an acquisition function <span class="math notranslate nohighlight">\(\alpha\)</span>. A popular criterion is, for example, the expected improvement (EI) that is the expectation of the new point returning a better solution than the previous best. Bayesian optimisation is performed in a loop where training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is used to fit the surrogate model before the next point suggested by the acquisition function is evaluated and added to the training data itself. The loop than restarts gaining more information about the objective function with each iteration. Bayesian optimisation is run for as many iterations as the evaluation budget $N$ allows, until a satisfying solution is found, or unitl a pre-defined stopping criterion is met.</p>
<blockquote>
<div><p><strong>Algorithm</strong></p>
<p>Specify evaluation budget <span class="math notranslate nohighlight">\(N\)</span>, number of initial points <span class="math notranslate nohighlight">\(n_0\)</span>, surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, acquisition function <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Sample <span class="math notranslate nohighlight">\(n_0\)</span> initial training data points <span class="math notranslate nohighlight">\(\boldsymbol X_0\)</span> via a space-filling design <a class="footnote-reference brackets" href="#id23" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> and gather observations <span class="math notranslate nohighlight">\(\boldsymbol y_0\)</span>.
Set <span class="math notranslate nohighlight">\(n = n_0\)</span> and <span class="math notranslate nohighlight">\(\mathcal{D}_n = \{ \boldsymbol X_0, \boldsymbol y_0 \}\)</span>.</p>
<p><strong>while</strong> <span class="math notranslate nohighlight">\(n \leq N\)</span> <strong>do:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Fit surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> to training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span>.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(x_n^*\)</span> that maximises an acquisition criterion <span class="math notranslate nohighlight">\(\alpha\)</span> based on model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>.</p></li>
<li><p>Evaluate <span class="math notranslate nohighlight">\(\boldsymbol x_n^*\)</span> observing <span class="math notranslate nohighlight">\(y_n^*\)</span> and add to <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span>.</p></li>
<li><p>Increment <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ol>
</div></blockquote>
<p><strong>end while</strong></p>
<p>Return point with highest observation <span class="math notranslate nohighlight">\(\boldsymbol x^*\)</span>.</p>
</div></blockquote>
</section>
<section id="surrogate-model">
<span id="model"></span><h2>Surrogate model<a class="headerlink" href="#surrogate-model" title="Permalink to this heading">#</a></h2>
<p>A popular choice for the surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> that acts as a representation of the objective function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span> is a Gaussian process <span class="math notranslate nohighlight">\(\mathcal{GP}\)</span> <a class="footnote-reference brackets" href="#id20" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id27" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>, a flexible non-parametric regression model. A Gaussian process is a finite collection of random variables that has a joint Gaussian distribution and is defined by a mean function <span class="math notranslate nohighlight">\(\mu_0(\boldsymbol x) : \mathcal{X} \mapsto \mathbb{R}\)</span> and a covariance kernel <span class="math notranslate nohighlight">\(\Sigma_0(\boldsymbol x, \boldsymbol x')  : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\)</span> resulting in the prior distribution</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[f(\boldsymbol X_n) \sim \mathcal{N} (m(\boldsymbol X_n), K(\boldsymbol X_n, \boldsymbol X_n)).\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(m(\boldsymbol X_n)\)</span> is the mean vector of size $n$ over all training inputs and <span class="math notranslate nohighlight">\(K(\boldsymbol X_n, \boldsymbol X_n)\)</span> is the <span class="math notranslate nohighlight">\(n \times n\)</span> covariance matrix between all training inputs.</p>
<p>The posterior or predictive distribution for <span class="math notranslate nohighlight">\(n_*\)</span> test points <span class="math notranslate nohighlight">\(\boldsymbol X_*\)</span> can be computed as the multivariate normal distribution conditional on some training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span></p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[f(\boldsymbol X_*) \mid \mathcal{D}_n, \boldsymbol X_* \sim \mathcal{N} \left(\mu_n (\boldsymbol X_*), \sigma^2_n (\boldsymbol X_*) \right)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mu_n (\boldsymbol X_*) = K(\boldsymbol X_*, \boldsymbol X_n) \left[ K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \right]^{-1} (\boldsymbol y - m (\boldsymbol X_n)) + m (\boldsymbol X_*)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\sigma^2_n (\boldsymbol X_*) = K (\boldsymbol X_*, \boldsymbol X_*) - K(\boldsymbol X_*, \boldsymbol X_n) \left[ K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \right]^{-1} K(\boldsymbol X_n, \boldsymbol X_*)\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(m(\boldsymbol X_*)\)</span> is the mean vector of size <span class="math notranslate nohighlight">\(n_*\)</span> over all test inputs, <span class="math notranslate nohighlight">\(K(\boldsymbol X_*, \boldsymbol X_n)\)</span> is the <span class="math notranslate nohighlight">\(n_* \times n\)</span>, <span class="math notranslate nohighlight">\(K(\boldsymbol X_n, \boldsymbol X_*)\)</span> is the <span class="math notranslate nohighlight">\(n \times n_*\)</span>, and <span class="math notranslate nohighlight">\(K(\boldsymbol X_*, \boldsymbol X_*)\)</span> is the <span class="math notranslate nohighlight">\(n_* \times n_*\)</span> covariance matrix between training inputs <span class="math notranslate nohighlight">\(\boldsymbol X_n\)</span> and test inputs <span class="math notranslate nohighlight">\(\boldsymbol X_*\)</span>.</p>
<p>Hyper-parameters of the Gaussian process such as any parameters <span class="math notranslate nohighlight">\(\theta\)</span> in the mean function and the covariance kernel or the noise variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> can be estimated by maximising the log marginal likelihood via maximum likelihood estimation (MLE):</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\log p(\boldsymbol y_n \mid \boldsymbol X_n) = -\frac{1}{2} (\boldsymbol y_n - m(\boldsymbol X_n))^T [K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I]^{-1} (\boldsymbol y_n - m(\boldsymbol X_n)) - \frac{1}{2} \log \lvert K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \rvert - \frac{n}{2} \log 2 \pi\]</div>
</div>
<p>NUBO uses the <code class="docutils literal notranslate"><span class="pre">GPyTorch</span></code> package <a class="footnote-reference brackets" href="#id19" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> for surrogate modelling. This is a very powerful package that allows the implementation of a wide selection of models ranging from exact Gaussian processes to approximate and even deep Gaussian processes. Besides maximum likelihood estimation (MLE) <code class="docutils literal notranslate"><span class="pre">GPyTorch</span></code> also supports maximum a posteriori estimation (MAP) and fully Bayesian estimation to estimate the hyper-parameter. It also comes with a rich documentation, many practical examples, and a large community.</p>
</section>
<section id="acquisition-function">
<span id="acquisition"></span><h2>Acquisition function<a class="headerlink" href="#acquisition-function" title="Permalink to this heading">#</a></h2>
<p>Acquisition functions use the posterior or predictive distribution of the Gaussian process <span class="math notranslate nohighlight">\(\mathcal{GP}\)</span> to compute a criterion that assess if a test point is good potential solution when evaluated through the objective function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span>. Thus, maximising the acquisition function suggests the test point that based on the current training data <span class="math notranslate nohighlight">\(\mathcal{D_n}\)</span> has the highest potential of being the global optimum. To do this, an acquisition function <span class="math notranslate nohighlight">\(\alpha\)</span> balances exploration and exploitation. The former characterised by areas that lack of observed data points and where the uncertainty of the Gaussian process is high, and the latter by promising areas with a high posterior mean of the Gaussian process. This exploration-exploitation trade-off ensures that Bayesian optimisation does not converge to the first (potentially local) maximum it finds but explores the full input space.</p>
<section id="analytical-acquisition-functions">
<h3>Analytical acquisition functions<a class="headerlink" href="#analytical-acquisition-functions" title="Permalink to this heading">#</a></h3>
<p>NUBO supports two of the most popular acquisition functions that are grounded in a rich history of theoretical and empirical research. Expected improvement (EI) <a class="footnote-reference brackets" href="#id21" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> selects points with the biggest potential of improving on the current best observation while upper confidence bound (UCB) <a class="footnote-reference brackets" href="#id26" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> takes an optimistic view of the posterior uncertainty and assumes a user-defined (through the hyper-parameter <span class="math notranslate nohighlight">\(\beta\)</span>) level of it to be true. Expected improvement (EI) is defined as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{EI} (\boldsymbol X_*) = \left(\mu_n(\boldsymbol X_*) - y^{best} \right) \Phi(z) + \sigma_n(\boldsymbol X_*) \phi(z)\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(z = \frac{\mu_n(\boldsymbol X_*) - y^{best}}{\sigma_n(\boldsymbol X_*)}\)</span>, <span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\sigma_n(\cdot)\)</span> are the mean and the standard deviation of the predictive distribution of the Gaussian process, $y^{best}$ is the current best observation, and <span class="math notranslate nohighlight">\(\Phi\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> are the cumulative distribution function and the probability density function of the standard normal distribution.
upper confidence bound (UCB) can be computed by</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{UCB} (\boldsymbol X_*) = \mu_n(\boldsymbol X_*) + \sqrt{\beta} \sigma_n(\boldsymbol X_*)\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a pre-defined trade-off parameter, and <span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\sigma_n(\cdot)\)</span> are the mean and the standard deviation of the predictive distribution of the Gaussian process.</p>
<p>Both of these acquisition functions can be computed analytically by maximising them with a deterministic optimiser such as L-BFGS-B for bounded unconstraint problems or SLSQP for bounded or constraint problems. However, this is only true for the sequential single-point case in which every points suggested by Bayesian optimisation is observed through the objective function <span class="math notranslate nohighlight">\(f( \boldsymbol x)\)</span> immediatley before the optimisation loop is repeated. NUBO contains a pre-specified Gaussian process with constant mean function and the Matern 5/2 covariance kernel that is especially suited for practical optimisation <a class="footnote-reference brackets" href="#id25" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="monte-carlo-acquisition-functions">
<h3>Monte Carlo acquisition functions<a class="headerlink" href="#monte-carlo-acquisition-functions" title="Permalink to this heading">#</a></h3>
<p>For parallel multi-point batches or asynchronous optimisation, the analytical acquisition functions are in general intractable. To allow Bayesian optimisation in these cases, NUBO supports the approximation of the analytical acquisition function through Monte Carlo sampling <a class="footnote-reference brackets" href="#id25" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id28" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>.</p>
<p>The idea is to draw a large number of samples directly from the predicitve distribution and then to approximate the acquisition by averaging these Monte Carlo samples. This method is made viable by reparameterising the acquisition functions and then computing samples from the predicitve distribution by utilising base samples from a standard normal distribution <span class="math notranslate nohighlight">\(z \sim \mathcal{N} (0, 1)\)</span>.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{EI}^{MC} (\boldsymbol X_*) = \max \left(ReLU(\mu_n(\boldsymbol X_*) + \boldsymbol L \boldsymbol z - y^{best}) \right)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{UCB}^{MC} (\boldsymbol X_*) = \max \left(\mu_n(\boldsymbol X_*) + \sqrt{\frac{\beta \pi}{2}} \lvert \boldsymbol L \boldsymbol z \rvert \right)\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> is the mean of the predictive distribution of the Gaussian process, <span class="math notranslate nohighlight">\(\boldsymbol L\)</span> is the lower triangular matrix of the Cholesky decomposition of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol L \boldsymbol L^T = K(\boldsymbol X_n, \boldsymbol X_n)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol z\)</span> are samples from the standard normal distribution, $y^{best}$ is the current best observation, <span class="math notranslate nohighlight">\(\beta\)</span> is the trade-off parameter, and <span class="math notranslate nohighlight">\(ReLU (\cdot)\)</span> is the rectified linear unit function that zeros all values below $0$ and leaves the rest as is.</p>
<p>Due to the randomness of the Monte Carlo samples, these acquisition functions can only be optimised by stochastic optimisers such as Adam <a class="footnote-reference brackets" href="#id22" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>. However, there is some empirical evidence that fixing the base samples for individual Bayesian optimisation loops does not affect the performance negatively. This method would allow deterministic optimiser to be used but could potentially introduce bias due to sampling randomness.</p>
<p>Furthermore, two optimisation strategies for batches are possible <a class="footnote-reference brackets" href="#id28" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>: The default is a joint optimisation approach where the acquisition functions are optimised over all points of the batch. The second option is a greedy sequential approach where one point after the other is selected holding each previous point fixed until the batch is full. Empirical evidence shows that both methods approximate the acquisition similarly for smaller batches. For larger batches, greedy optimisation performs better as the optimisation of the joint approach increases in complexity with the batch size.</p>
<p>Asynchronous optimisation <a class="footnote-reference brackets" href="#id25" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> leverages the same property as sequential greedy optimisation: the pending points that have not yet been evaluated can be added to the test points but are treated as fixed. In this way, they affect the joint multivariate normal distribution but are not considered directly in the optimisation.</p>
<a class="reference internal image-reference" href="_images/unnamed.png"><img alt="_images/unnamed.png" src="_images/unnamed.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/unnamed-2.png"><img alt="_images/unnamed-2.png" src="_images/unnamed-2.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/unnamed-3.png"><img alt="_images/unnamed-3.png" src="_images/unnamed-3.png" style="width: 49%;" /></a>
<a class="reference internal image-reference" href="_images/unnamed-4.png"><img alt="_images/unnamed-4.png" src="_images/unnamed-4.png" style="width: 49%;" /></a>
<p>Figure 1: Bayesian optimisation example. A Gaussian process is fitted to three
initial observations (dark blue dots) resulting in the posterior mean (solid
red line) and the posterior variance represented here as the 95% confidence
interval (blue area). The expected improvement (EI) acquisition function
(orange area) is maximised to find the next point that should be observed
(dashed black line) from the objective function. Once observed, the input and
output are added to the training data and the process is repeated two more
times. The final Gaussian process model is than compared to the true objective
function (solid black line). The last evaluated point approximates the
maximum.</p>
<hr class="docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id18" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>PI Frazier, “A tutorial on Bayesian optimization,” <em>arXiv preprint arXiv:1807.02811</em>, 2018.</p>
</aside>
<aside class="footnote brackets" id="id19" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">2</a><span class="fn-bracket">]</span></span>
<p>J Gardner, G Pleiss, KQ Weinberger, D Bindel, and AG Wilson, “GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration,” <em>Advances in neural information processing systems</em>, vol. 31, 2018.</p>
</aside>
<aside class="footnote brackets" id="id20" role="note">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id7">2</a>)</span>
<p>RB Gramacy, <em>Surrogates: Gaussian process modeling, design, and optimization for the applied sciences</em>, 1st ed. Boca Raton, FL: CRC press, 2020.</p>
</aside>
<aside class="footnote brackets" id="id21" role="note">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>DR Jones, M Schonlau, and WJ Welch, “Efficient global optimization of expensive black-box functions,” <em>Journal of Global optimization</em>, vol. 13, no. 4, p. 566, 1998.</p>
</aside>
<aside class="footnote brackets" id="id22" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">5</a><span class="fn-bracket">]</span></span>
<p>DP Kingma and J Ba, “Adam: A method for stochastic optimization,” <em>Proceedings of the 3rd International Conference on Learning Representations</em>, 2015.</p>
</aside>
<aside class="footnote brackets" id="id23" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>MD McKay, RJ Beckman, and WJ Conover, “A comparison of three methods for selecting values of input variables in the analysis of output from a computer code,” <em>Technometrics</em>, vol. 42, no. 1, p. 55-61, 2000.</p>
</aside>
<aside class="footnote brackets" id="id24" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">7</a><span class="fn-bracket">]</span></span>
<p>B Shahriari, K Swersky, Z Wang, RP Adams, and N De Freitas, “Taking the human out of the loop: A review of Bayesian optimization,” <em>Proceedings of the IEEE</em>, vol. 104, no. 1, p. 148-175, 2015.</p>
</aside>
<aside class="footnote brackets" id="id25" role="note">
<span class="label"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id5">1</a>,<a role="doc-backlink" href="#id12">2</a>,<a role="doc-backlink" href="#id13">3</a>,<a role="doc-backlink" href="#id17">4</a>)</span>
<p>J Snoek, H Larochelle, and RP Adams, “Practical bayesian optimization of machine learning algorithms,” <em>Advances in neural information processing systems</em>, vol. 25, 2012.</p>
</aside>
<aside class="footnote brackets" id="id26" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">9</a><span class="fn-bracket">]</span></span>
<p>N Srinivas, A Krause, SM Kakade, and M Seeger, “Gaussian process optimization in the bandit setting: No regret and experimental design,” <em>Proceedings of the 27th International Conference on Machine Learning</em>, p. 1015-1022, 2010.</p>
</aside>
<aside class="footnote brackets" id="id27" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">10</a><span class="fn-bracket">]</span></span>
<p>CKI Williams, and CE Rasmussen, <em>Gaussian processes for machine learning</em>, 2nd ed. Cambridge, MA: MIT press, 2006.</p>
</aside>
<aside class="footnote brackets" id="id28" role="note">
<span class="label"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id14">1</a>,<a role="doc-backlink" href="#id16">2</a>)</span>
<p>J Wilson, F Hutter, and M Deisenroth, “Maximizing acquisition functions for Bayesian optimization,” <em>Advances in neural information processing systems</em>, vol. 31, 2018.</p>
</aside>
</aside>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="citation.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Citation</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="get_started.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Get started</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Mike Diessner
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">A primer on Bayesian optimisation</a><ul>
<li><a class="reference internal" href="#maximisation-problem">Maximisation problem</a></li>
<li><a class="reference internal" href="#bayesian-optimisation">Bayesian optimisation</a></li>
<li><a class="reference internal" href="#surrogate-model">Surrogate model</a></li>
<li><a class="reference internal" href="#acquisition-function">Acquisition function</a><ul>
<li><a class="reference internal" href="#analytical-acquisition-functions">Analytical acquisition functions</a></li>
<li><a class="reference internal" href="#monte-carlo-acquisition-functions">Monte Carlo acquisition functions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>