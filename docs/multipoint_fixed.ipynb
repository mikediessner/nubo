{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel multi-point Bayesian optimisation with fixed base samples\n",
    "This notebook shows how NUBO can perform parallel multi-point optimisation with fixed base samples. This enables the use of an deterministic optimiser, such as L-BFGS-B and SLSQP, and parallel constrained optimisation.\n",
    "\n",
    "In the first example below, NUBO is used to perform multi-point optimisation that allows the candidates to be evaluated from the objective function in parallel. Multi-point optimisation is implemented in NUBO through Monte Carlo acquisition functions. The script below uses the `MCUpperConfidenceBound` acquisition function with 512 samples and fixed base samples. Each batch of 4 is found jointly with the `multi_joint()` function by optimising the acquisition function with the deterministic L-BFGS-B optimiser. The `Hartmann6D` synthetic test function acts as a surrogate for a black box objective funtion, such as an experiment or a simulation. The optimisation loop is run for 10 iterations returning batches of 4 each (a total of 40 evaluations) and finds a solution close the true optimum of -3.3224. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 36: \t Inputs: [0.     0.     0.     0.1476 0.276  0.6154], \t Outputs: -1.5197\n",
      "New best at evaluation 45: \t Inputs: [0.     0.1924 0.1315 0.2455 0.2446 0.6663], \t Outputs: -2.1614\n",
      "New best at evaluation 48: \t Inputs: [0.     0.1833 0.6835 0.2924 0.2919 0.6498], \t Outputs: -2.4447\n",
      "New best at evaluation 55: \t Inputs: [0.3088 0.1569 0.6135 0.2713 0.2833 0.6521], \t Outputs: -2.9539\n",
      "New best at evaluation 65: \t Inputs: [0.3024 0.1871 0.46   0.2855 0.304  0.6988], \t Outputs: -3.1494\n",
      "New best at evaluation 70: \t Inputs: [0.218  0.1662 0.4164 0.2816 0.3224 0.6443], \t Outputs: -3.2737\n",
      "Evaluation: 70 \t Solution: -3.2737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.acquisition import MCExpectedImprovement, MCUpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import multi_joint, multi_sequential\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 10\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = MCExpectedImprovement(gp=gp, y_best=torch.max(y_train), samples=512, fix_base_samples=True)\n",
    "    acq = MCUpperConfidenceBound(gp=gp, beta=1.96**2, samples=512, fix_base_samples=True)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = multi_joint(func=acq, method=\"L-BFGS-B\", batch_size=4, bounds=bounds, num_starts=5)\n",
    "    # x_new, _ = multi_sequential(func=acq, method=\"L-BFGS-B\", batch_size=4, bounds=bounds, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if torch.max(y_new) > torch.max(y_train[:-y_new.size(0)]):\n",
    "        best_eval = torch.argmax(y_train)\n",
    "        print(f\"New best at evaluation {best_eval+1}: \\t Inputs: {x_train[best_eval, :].numpy().reshape(dims).round(4)}, \\t Outputs: {-y_train[best_eval].numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained optimisation\n",
    "In the second example,  NUBO is used to maximise a function where the input space is bounded and constrained. The whole process is not too different from the unconstrained case. We only need to choose a different optimiser that allows the use of constraints when maximising the acquisition function `MCUpperConfidenceBound` with fixed base samples. At the moment parallel constrained optimisation is only supported by the sequential greedy optimisation strategy using `multi_sequential()`. NUBO uses the SLSQP optimiser that can be provided with a dictionary or a tuple of dictionaries that specify one or multiple constraints. We specify two constraints to showcase the two different options: equality constraints and inequality constraints. Equality constraints require the constraint to be 0 while the result is non-negative for inequality constraints. Our first constraint `{'type': 'ineq', 'fun': lambda x: 0.5 - x[0] - x[1]}` is an inequality constraint and requires the sum of the first two inputs to be smaller or equal to 0.5. The second constraint `{'type': 'eq', 'fun': lambda x: 1.2442 - x[3] - x[4] - x[5]}` is an equality constraint specifying that the sum of the last three inputs needs to be equal to 1.2442. The `Hartmann6D` synthetic test function acts as a substitute for a black box objective funtion, such as an experiment or a simulation. The optimisation loop is run for 40 iterations and finds a solution close the true optimum of -3.3224. Important: Generating initial input points with a Latin hypercube might not work for real problems as they will not consider the constraints but only the bounds. In these situations, other methods or selecting initial points by hand might be preferable. The purpose of this example is solely the demonstration of how NUBO handles constraints and constrained optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 31: \t Inputs: [0.2252 0.     0.5246 0.3175 0.2892 0.6376], \t Outputs: -2.9618\n",
      "New best at evaluation 47: \t Inputs: [0.3662 0.1338 0.4741 0.312  0.2949 0.6373], \t Outputs: -2.9727\n",
      "New best at evaluation 53: \t Inputs: [0.259  0.241  0.5032 0.2755 0.3047 0.664 ], \t Outputs: -3.1813\n",
      "New best at evaluation 55: \t Inputs: [0.2064 0.1458 0.4293 0.2751 0.3058 0.6633], \t Outputs: -3.3008\n",
      "New best at evaluation 59: \t Inputs: [0.206  0.1526 0.4764 0.2806 0.3169 0.6467], \t Outputs: -3.3169\n",
      "New best at evaluation 63: \t Inputs: [0.21   0.1482 0.486  0.2736 0.3104 0.6602], \t Outputs: -3.3201\n",
      "Evaluation: 63 \t Solution: -3.3201\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.acquisition import MCExpectedImprovement, MCUpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import multi_sequential\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds and constraints\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "cons = ({'type': 'ineq', 'fun': lambda x: 0.5 - x[0] - x[1]},\n",
    "        {'type': 'eq', 'fun': lambda x: 1.2442 - x[3] - x[4] - x[5]})\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 10\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = MCExpectedImprovement(gp=gp, y_best=torch.max(y_train), samples=512, fix_base_samples=True)\n",
    "    acq = MCUpperConfidenceBound(gp=gp, beta=1.96**2, samples=512, fix_base_samples=True)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = multi_sequential(func=acq, method=\"SLSQP\", batch_size=4, bounds=bounds, constraints=cons, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if torch.max(y_new) > torch.max(y_train[:-y_new.size(0)]):\n",
    "        best_eval = torch.argmax(y_train)\n",
    "        print(f\"New best at evaluation {best_eval+1}: \\t Inputs: {x_train[best_eval, :].numpy().reshape(dims).round(4)}, \\t Outputs: {-y_train[best_eval].numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
