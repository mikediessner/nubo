<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Practical considerations" href="practical_considerations.html" /><link rel="prev" title="Get started" href="get_started.html" />

    <link rel="shortcut icon" href="_static/favicon.ico"/><!-- Generated with Sphinx 6.1.3 and Furo 2022.12.07 -->
        <title>A primer on Bayesian optimisation - NUBO</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=91d0f0d1c444bdcb17a68e833c7a53903343c195" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">NUBO</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">NUBO</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">NUBO:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Get started</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">A primer on Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="practical_considerations.html">Practical considerations</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Citation</a></li>
<li class="toctree-l1"><a class="reference external" href="http://github.com/mikediessner/nubo">Github</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_gp.html">Custom Gaussian process</a></li>
<li class="toctree-l1"><a class="reference internal" href="singlepoint.html">Sequential single-point Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="multipoint_joint.html">Parallel multi-point joint Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="multipoint_sequential.html">Parallel multi-point sequential Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="multipoint_fixed.html">Parallel multi-point Bayesian optimisation with fixed base samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="asynchronous_bo.html">Asynchronous Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="constrained_bo.html">Constrained single-point Bayesian optimisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mixed_parameters.html">Bayesian optimisation with continuous and discrete parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="fixed_noise.html">Bayesian optimisation with known observational noise</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="nubo.acquisition.html">Acquisition module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.models.html">Surrogate module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.optimisation.html">Optimisation module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.test_functions.html">Test function module</a></li>
<li class="toctree-l1"><a class="reference internal" href="nubo.utils.html">Utility module</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="a-primer-on-bayesian-optimisation">
<span id="bo"></span><h1>A primer on Bayesian optimisation<a class="headerlink" href="#a-primer-on-bayesian-optimisation" title="Permalink to this heading">#</a></h1>
<p>The following introduction aims to give a concise explanation of the Bayesian
optimisation algorithm and its element, such as the surrogate model and the
acquisition functions. While this introduction covers all critical details and
will be sufficient to get started with Bayesian optimisation and understand how
NUBO works, it should not be considered as exhaustive. Where appropriate,
references highlight resources for additional reading that will present a more
detailed picture of Bayesian optimisation than is possible here.</p>
<section id="maximisation-problem">
<span id="objfunc"></span><h2>Maximisation problem<a class="headerlink" href="#maximisation-problem" title="Permalink to this heading">#</a></h2>
<p>Bayesian optimisation aims to solve the <span class="math notranslate nohighlight">\(d\)</span>-dimensional maximisation
problem</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\boldsymbol  x^* = \arg \max_{\boldsymbol  x \in \mathcal{X}} f(\boldsymbol x),\]</div>
</div>
<p>where the input space is usually continuous and bounded by a hyper-rectangle
<span class="math notranslate nohighlight">\(\mathcal{X} \in [a, b]^d\)</span> with <span class="math notranslate nohighlight">\(a, b \in \mathbb{R}\)</span>. The function
<span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span> is most commonly a derivative-free
expensive-to-evaluate black box problem that only allows inputs
<span class="math notranslate nohighlight">\(\boldsymbol x_i\)</span> to be queried and outputs <span class="math notranslate nohighlight">\(y_i\)</span> to be observed
without gaining any further insights into the underlying system. We assume any
noise <span class="math notranslate nohighlight">\(\epsilon\)</span> that is introduced when taking measurements to be
independent and identically distributed Gaussian noise
<span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N} (0, \sigma^2)\)</span> such that
<span class="math notranslate nohighlight">\(y_i = f(\boldsymbol  x_i) + \epsilon\)</span>. Hence, a set of <span class="math notranslate nohighlight">\(n\)</span> pairs
of data points and corresponding observations is defined as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathcal{D_n} = \{(\boldsymbol x_i, y_i)\}_{i=1}^n\]</div>
</div>
<p>and we further define training inputs as matrix
<span class="math notranslate nohighlight">\(\boldsymbol X_n = \{\boldsymbol x_i \}_{i=1}^n\)</span> and their training
outputs as vector <span class="math notranslate nohighlight">\(\boldsymbol y_n = \{y_i\}_{i=1}^n\)</span>.</p>
<p>Many simulations and experiments in various disciplines can be formulated to
fit this discription. For example, Bayesian optimisation was used in the field
of computational fluid dynamics to maximise the drag reduction via active
control of blowing actuators <a class="footnote-reference brackets" href="#diessner2022" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#oconnor2023" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#mahfoze2019" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>,
in chemical engineering for molecular design, drug discovery, molecular
modeling, electrolyte design, and additive manufacturing <a class="footnote-reference brackets" href="#wang2022" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a>, and in
computer science to fine-tune hyper-parameters of machine learning models
<a class="footnote-reference brackets" href="#wu2019" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a> or in architecture search for neural networks <a class="footnote-reference brackets" href="#white2021" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>.</p>
</section>
<section id="bayesian-optimisation">
<h2>Bayesian optimisation<a class="headerlink" href="#bayesian-optimisation" title="Permalink to this heading">#</a></h2>
<p>Bayesian optimisation <a class="footnote-reference brackets" href="#frazier2018" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#gramacy2020" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#jones1998" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>
<a class="footnote-reference brackets" href="#shahriari2015" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#snoek2012" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a> is a surrogate model-based optimisation
algorithm that aims to maximise the objective function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span>
in a minimum number of function evaluations. Usually, the objective function
does not have a known mathematical expression and every function evaluation is
expensive. Such problems require a cost-effective and sample-efficient
optimisation strategy. Bayesian optimisation meets these criteria by
representing the objective function through a surrogate model
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, often a Gaussian process <span class="math notranslate nohighlight">\(\mathcal{GP}\)</span>. This
representation can then be used to find the next point that should be evaluated
by maximising a criterion specified through an acquisition function
<span class="math notranslate nohighlight">\(\alpha (\cdot)\)</span>. A popular criterion is, for example, the expected
improvement (EI) that is the expectation of the new point returning a better
solution than the previous best observation. Bayesian optimisation is performed
in a loop where training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is used to fit the
surrogate model before the next point suggested by the acquisition function is
evaluated and added to the training data itself (see algorithm below). The loop
than restarts gathering more information about the objective function with each
iteration. Bayesian optimisation is run for as many iterations as the
evaluation budget <span class="math notranslate nohighlight">\(N\)</span> allows, until a satisfying solution is found, or
unitl a pre-defined stopping criterion is met.</p>
<div class="seealso admonition">
<p class="admonition-title">Algorithm</p>
<p>Specify evaluation budget <span class="math notranslate nohighlight">\(N\)</span>, number of initial points <span class="math notranslate nohighlight">\(n_0\)</span>, surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>, acquisition function <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>Sample <span class="math notranslate nohighlight">\(n_0\)</span> initial training data points <span class="math notranslate nohighlight">\(\boldsymbol X_0\)</span> via a space-filling design <a class="footnote-reference brackets" href="#mckay2000" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> and gather observations <span class="math notranslate nohighlight">\(\boldsymbol y_0\)</span>.</p>
<p>Set <span class="math notranslate nohighlight">\(\mathcal{D}_n = \{ \boldsymbol X_0, \boldsymbol y_0 \}\)</span>.</p>
<p><strong>while</strong> <span class="math notranslate nohighlight">\(n \leq N -n_0\)</span> <strong>do:</strong></p>
<ol class="arabic simple">
<li><p>Fit surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> to training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span>.</p></li>
<li><p>Find <span class="math notranslate nohighlight">\(\boldsymbol x_n^*\)</span> that maximises an acquisition criterion <span class="math notranslate nohighlight">\(\alpha\)</span> based on model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>.</p></li>
<li><p>Evaluate <span class="math notranslate nohighlight">\(\boldsymbol x_n^*\)</span> observing <span class="math notranslate nohighlight">\(y_n^*\)</span> and add to <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span>.</p></li>
<li><p>Increment <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ol>
<p><strong>end while</strong></p>
<p>Return point <span class="math notranslate nohighlight">\(\boldsymbol x^*\)</span> with highest observation.</p>
</div>
<p>The animation below illustrates how the Bayesian optimisation algorithm works
on an optimisation loop that runs for 20 iterations. The surrogate model uses
the available observaions to provide a prediction and its uncertainty (here
shown as 95% confidence intervals around the prediction). This is our best
guess of the underlying objective function. This guess is than used in the
acquisition function to evaluate which point is most likely to improve over the
current best solution. Maximising the acquisition yields the next candidate
that is observed from the objective function, i.e. the truth, before it is
added to the training data and the whole process is repeated again. The
animation shows how the surrogate model gets closer to the truth with each
iteration and how the acquisition function explores the input space by
exploring regions with a high uncertainty and exploits regions with a high
prediction. This property also called the exploration-exploitation trade-off
is a corner stone of the acquisition functions provided in NUBO.</p>
<figure class="align-default" id="id28">
<img alt="_images/bo.gif" src="_images/bo.gif" />
<figcaption>
<p><span class="caption-text">Figure 1: Bayesian optimisation of a 1D toy function with a budget of
20 evaluations.</span><a class="headerlink" href="#id28" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="surrogate-model">
<span id="model"></span><h2>Surrogate model<a class="headerlink" href="#surrogate-model" title="Permalink to this heading">#</a></h2>
<p>A popular choice for the surrogate model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> that acts as a
representation of the objective function <span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span> is a Gaussian
process <span class="math notranslate nohighlight">\(\mathcal{GP}\)</span> <a class="footnote-reference brackets" href="#gramacy2020" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#williams2006" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a>, a flexible
non-parametric regression model. A Gaussian process is a finite collection of
random variables that has a joint Gaussian distribution and is defined by a
prior mean function
<span class="math notranslate nohighlight">\(\mu_0(\boldsymbol x) : \mathcal{X} \mapsto \mathbb{R}\)</span> and a prior
covariance kernel
<span class="math notranslate nohighlight">\(\Sigma_0(\boldsymbol x, \boldsymbol x')  : \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}\)</span>
resulting in the prior distribution</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[f(\boldsymbol X_n) \sim \mathcal{N} (m(\boldsymbol X_n), K(\boldsymbol X_n, \boldsymbol X_n)),\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(m(\boldsymbol X_n)\)</span> is the mean vector of size <span class="math notranslate nohighlight">\(n\)</span> over all
training inputs and <span class="math notranslate nohighlight">\(K(\boldsymbol X_n, \boldsymbol X_n)\)</span> is the
<span class="math notranslate nohighlight">\(n \times n\)</span> covariance matrix between all training inputs.</p>
<p>The posterior distribution for <span class="math notranslate nohighlight">\(n_*\)</span> test points <span class="math notranslate nohighlight">\(\boldsymbol X_*\)</span>
can be computed as the multivariate normal distribution conditional on some
training data <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span></p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[f(\boldsymbol X_*) \mid \mathcal{D}_n, \boldsymbol X_* \sim \mathcal{N} \left(\mu_n (\boldsymbol X_*), \sigma^2_n (\boldsymbol X_*) \right)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mu_n (\boldsymbol X_*) = K(\boldsymbol X_*, \boldsymbol X_n) \left[ K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \right]^{-1} (\boldsymbol y - m (\boldsymbol X_n)) + m (\boldsymbol X_*)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\sigma^2_n (\boldsymbol X_*) = K (\boldsymbol X_*, \boldsymbol X_*) - K(\boldsymbol X_*, \boldsymbol X_n) \left[ K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \right]^{-1} K(\boldsymbol X_n, \boldsymbol X_*),\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(m(\boldsymbol X_*)\)</span> is the mean vector of size <span class="math notranslate nohighlight">\(n_*\)</span> over
all test inputs, <span class="math notranslate nohighlight">\(K(\boldsymbol X_*, \boldsymbol X_n)\)</span> is the
<span class="math notranslate nohighlight">\(n_* \times n\)</span>, <span class="math notranslate nohighlight">\(K(\boldsymbol X_n, \boldsymbol X_*)\)</span> is the
<span class="math notranslate nohighlight">\(n \times n_*\)</span>, and <span class="math notranslate nohighlight">\(K(\boldsymbol X_*, \boldsymbol X_*)\)</span> is the
<span class="math notranslate nohighlight">\(n_* \times n_*\)</span> covariance matrix between training inputs
<span class="math notranslate nohighlight">\(\boldsymbol X_n\)</span> and test inputs <span class="math notranslate nohighlight">\(\boldsymbol X_*\)</span>.</p>
<p>Hyper-parameters of the Gaussian process, such as any parameters <span class="math notranslate nohighlight">\(\theta\)</span>
in the mean function and the covariance kernel or the noise variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span>, can be estimated by maximising the log marginal likelihood
below via maximum likelihood estimation (MLE).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\log p(\boldsymbol y_n \mid \boldsymbol X_n) = -\frac{1}{2} (\boldsymbol y_n - m(\boldsymbol X_n))^T [K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I]^{-1} (\boldsymbol y_n - m(\boldsymbol X_n)) - \frac{1}{2} \log \lvert K(\boldsymbol X_n, \boldsymbol X_n) + \sigma^2 I \rvert - \frac{n}{2} \log 2 \pi\]</div>
</div>
<figure class="align-default" id="id29">
<img alt="_images/gp.gif" src="_images/gp.gif" />
<figcaption>
<p><span class="caption-text">Figure 2: Change of Gaussian process model (prediction and
corresponding uncertainty) over 20 iterations.</span><a class="headerlink" href="#id29" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>NUBO uses the <code class="docutils literal notranslate"><span class="pre">GPyTorch</span></code> package <a class="footnote-reference brackets" href="#gardner2018" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> for surrogate modelling.
This is a very powerful package that allows the implementation of a wide
selection of models ranging from exact Gaussian processes to approximate and
even deep Gaussian processes. Besides maximum likelihood estimation (MLE)
<code class="docutils literal notranslate"><span class="pre">GPyTorch</span></code> also supports maximum a posteriori estimation (MAP) and fully
Bayesian estimation to estimate the hyper-parameter. It also comes with a rich
documentation, many practical examples, and a large community.</p>
<p>NUBO provides a Gaussian process for off-the-shelf use with a constant mean
function and a Matern 5/2 covariance kernel that due to its flexibility is
especially suited for practical optimisation <a class="footnote-reference brackets" href="#snoek2012" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>. A tutorial on how
to implement a custom Gaussian process to use with NUBO can be found in the
examples section. For more complex models we recommend consulting the
<code class="docutils literal notranslate"><span class="pre">GPyTorch</span></code> <a class="reference external" href="https://docs.gpytorch.ai/en/stable">documentation</a>.</p>
</section>
<section id="acquisition-function">
<span id="acquisition"></span><h2>Acquisition function<a class="headerlink" href="#acquisition-function" title="Permalink to this heading">#</a></h2>
<p>Acquisition functions use the posterior distribution of the Gaussian process
<span class="math notranslate nohighlight">\(\mathcal{GP}\)</span> to compute a criterion that assess if a test point is a
good potential candidate point when evaluated through the objective function
<span class="math notranslate nohighlight">\(f(\boldsymbol x)\)</span>. Thus, maximising the acquisition function suggests
the test point that based on the current training data <span class="math notranslate nohighlight">\(\mathcal{D_n}\)</span>
has the highest potential of being the global optimum. To do this, an
acquisition function <span class="math notranslate nohighlight">\(\alpha (\cdot)\)</span> balances exploration and
exploitation. The former is characterised by areas with no or only a few
observed data points where the uncertainty of the Gaussian process is high, and
the latter by areas where the posterior mean of the Gaussian process is high.
This exploration-exploitation trade-off ensures that Bayesian optimisation does
not converge to the first (potentially local) maximum it encounters but
gradually explores the full input space.</p>
<section id="analytical-acquisition-functions">
<h3>Analytical acquisition functions<a class="headerlink" href="#analytical-acquisition-functions" title="Permalink to this heading">#</a></h3>
<p>NUBO supports two of the most popular acquisition functions that are grounded
in a rich history of theoretical and empirical research. Expected improvement
(EI) <a class="footnote-reference brackets" href="#jones1998" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> selects points with the biggest potential of improving on
the current best observation while upper confidence bound (UCB)
<a class="footnote-reference brackets" href="#srinivas2010" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a> takes an optimistic view of the posterior uncertainty and
assumes it to be true to a user-defined level. Expected improvement (EI) is
defined as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{EI} (\boldsymbol X_*) = \left(\mu_n(\boldsymbol X_*) - y^{best} \right) \Phi(z) + \sigma_n(\boldsymbol X_*) \phi(z),\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(z = \frac{\mu_n(\boldsymbol X_*) - y^{best}}{\sigma_n(\boldsymbol X_*)}\)</span>,
<span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\sigma_n(\cdot)\)</span> are the mean and the standard
deviation of the posterior distribution of the Gaussian process,
<span class="math notranslate nohighlight">\(y^{best}\)</span> is the current best observation, and <span class="math notranslate nohighlight">\(\Phi (\cdot)\)</span> and
<span class="math notranslate nohighlight">\(\phi  (\cdot)\)</span> are the cumulative distribution function and the
probability density function of the standard normal distribution.</p>
<figure class="align-default" id="id30">
<img alt="_images/bo_ei.gif" src="_images/bo_ei.gif" />
<figcaption>
<p><span class="caption-text">Figure 3: Bayesian optimisation using the analytical expected
improvement acquisition function of a 1D toy function with a budget of
20 evaluations.</span><a class="headerlink" href="#id30" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The upper confidence bound (UCB) acquisition function can be computed as</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{UCB} (\boldsymbol X_*) = \mu_n(\boldsymbol X_*) + \sqrt{\beta} \sigma_n(\boldsymbol X_*),\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a pre-defined trade-off parameter, and
<span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\sigma_n(\cdot)\)</span> are the mean and the standard
deviation of the posterior distribution of the Gaussian process. The animation
below shows how the acquisition would look when <span class="math notranslate nohighlight">\(\beta\)</span> is set to 16. For
comparison, the posterior uncertainty shown as the 95% confidence interval
around the posterior mean of the Gaussian process is equal to using
<span class="math notranslate nohighlight">\(\beta = 1.96^2\)</span>.</p>
<figure class="align-default" id="id31">
<img alt="_images/bo_ucb.gif" src="_images/bo_ucb.gif" />
<figcaption>
<p><span class="caption-text">Figure 4: Bayesian optimisation using the analytical upper confidence
bound acquisition function of a 1D toy function with a budget of
20 evaluations.</span><a class="headerlink" href="#id31" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Both of these acquisition functions can be computed analytically by maximising
them with a deterministic optimiser, such as L-BFGS-B <a class="footnote-reference brackets" href="#zhu1997" id="id19" role="doc-noteref"><span class="fn-bracket">[</span>20<span class="fn-bracket">]</span></a> for bounded
unconstraint problems or SLSQP <a class="footnote-reference brackets" href="#kraft1994" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> for bounded constraint problems.
However, this only works for the sequential single-point problems for which
every point suggested by Bayesian optimisation is observed through the
objective function <span class="math notranslate nohighlight">\(f( \boldsymbol x)\)</span> immediatley before the
optimisation loop is repeated.</p>
</section>
<section id="monte-carlo-acquisition-functions">
<h3>Monte Carlo acquisition functions<a class="headerlink" href="#monte-carlo-acquisition-functions" title="Permalink to this heading">#</a></h3>
<p>For parallel multi-point batches or asynchronous optimisation, the analytical
acquisition functions are in general intractable. To use Bayesian
optimisation in these cases, NUBO supports the approximation of the analytical
acquisition function through Monte Carlo sampling <a class="footnote-reference brackets" href="#snoek2012" id="id21" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#wilson2018" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>.</p>
<p>The idea is to draw a large number of samples directly from the posterior
distribution and then to approximate the acquisition by averaging these so
called Monte Carlo samples. This method is made viable by reparameterising the
acquisition functions and then computing samples from the posterior
distribution by utilising base samples from a standard normal distribution
<span class="math notranslate nohighlight">\(z \sim \mathcal{N} (0, 1)\)</span>.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{EI}^{MC} (\boldsymbol X_*) = \max \left(ReLU(\mu_n(\boldsymbol X_*) + \boldsymbol L \boldsymbol z - y^{best}) \right)\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\alpha_{UCB}^{MC} (\boldsymbol X_*) = \max \left(\mu_n(\boldsymbol X_*) + \sqrt{\frac{\beta \pi}{2}} \lvert \boldsymbol L \boldsymbol z \rvert \right),\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\mu_n(\cdot)\)</span> is the mean of the predictive distribution of the
Gaussian process, <span class="math notranslate nohighlight">\(\boldsymbol L\)</span> is the lower triangular matrix of the
Cholesky decomposition of the covariance matrix
<span class="math notranslate nohighlight">\(\boldsymbol L \boldsymbol L^T = K(\boldsymbol X_n, \boldsymbol X_n)\)</span>,
<span class="math notranslate nohighlight">\(\boldsymbol z\)</span> are samples from the standard normal distribution
<span class="math notranslate nohighlight">\(\mathcal{N} (0, 1)\)</span>, <span class="math notranslate nohighlight">\(y^{best}\)</span> is the current best observation,
<span class="math notranslate nohighlight">\(\beta\)</span> is the trade-off parameter, and <span class="math notranslate nohighlight">\(ReLU (\cdot)\)</span> is the
rectified linear unit function that zeros all values below 0 and leaves the
rest as is.</p>
<p>Due to the randomness of the Monte Carlo samples, these acquisition functions
can only be optimised by stochastic optimisers, such as Adam <a class="footnote-reference brackets" href="#kingma2015" id="id23" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>.
However, there is some empirical evidence that fixing the base samples for
individual Bayesian optimisation loops does not affect the performance
negatively <a class="footnote-reference brackets" href="#balandat2020" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>. This method would allow deterministic optimiser to
be used but could potentially introduce bias due to sampling randomness. NUBO
lets you decide which variant you prefer by setting <code class="docutils literal notranslate"><span class="pre">fix_base_samples</span></code> and
choosing the prefered optimiser. Bounded problems can be solved with Adam
(<code class="docutils literal notranslate"><span class="pre">fix_base_samples</span> <span class="pre">=</span> <span class="pre">False</span></code>) or L-BFGS-B (<code class="docutils literal notranslate"><span class="pre">fix_base_samples</span> <span class="pre">=</span> <span class="pre">True</span></code>) and
constraint problems can be solved with SLSQP (<code class="docutils literal notranslate"><span class="pre">fix_base_samples</span> <span class="pre">=</span> <span class="pre">True</span></code>).</p>
<p>Furthermore, two optimisation strategies for batches are possible
<a class="footnote-reference brackets" href="#wilson2018" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>: The default is a joint optimisation approach where the
acquisition functions are optimised over all points of the batch
simultaneously. The second option is a greedy sequential approach where one
point after the other is selected holding all previous points fixed until the
batch is full. Empirical evidence shows that both methods approximate the
acquisition successfully. However, the greedy approach seems to have a slight
edge over the joint strategy for some examples <a class="footnote-reference brackets" href="#wilson2018" id="id26" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a>. It is also
faster to compute for larger batches. At the moment, constrained optimisation
with SLSQP is only supported for the sequential strategy.</p>
<p>Asynchronous optimisation <a class="footnote-reference brackets" href="#snoek2012" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a> leverages the same property as
sequential greedy optimisation: the pending points that have not yet been
evaluated can be added to the test points but are treated as fixed. In this
way, they affect the joint multivariate normal distribution but are not
considered directly in the optimisation.</p>
<figure class="align-default" id="id32">
<a class="reference external image-reference" href="https://mikediessner.github.io/nubo/_build/html/_images/flowchart.png"><img alt="_images/flowchart.png" src="_images/flowchart.png" /></a>
<figcaption>
<p><span class="caption-text">Figure 5: Flowchart to determine what Bayesian optimisation algorithm is recommended.
Click to expand.</span><a class="headerlink" href="#id32" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="balandat2020" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">1</a><span class="fn-bracket">]</span></span>
<p>M Balandat <em>et al.</em>, “BoTorch: A framework for efficient Monte-CarloBayesian optimization,” <em>Advances in neural information processing systems</em>, vol. 33, 2020.</p>
</aside>
<aside class="footnote brackets" id="diessner2022" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">2</a><span class="fn-bracket">]</span></span>
<p>M Diessner, J O’Connor, A Wynn, S Laizet, Y Guan, K Wilson, and R D Whalley, “Investigating Bayesian optimization for expensive-to-evaluate black box functions: Application in fluid dynamics,” <em>Frontiers in Applied Mathematics and Statistics</em>, 2022.</p>
</aside>
<aside class="footnote brackets" id="frazier2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">3</a><span class="fn-bracket">]</span></span>
<p>P I Frazier, “A tutorial on Bayesian optimization,” <em>arXiv preprint arXiv:1807.02811</em>, 2018.</p>
</aside>
<aside class="footnote brackets" id="gardner2018" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">4</a><span class="fn-bracket">]</span></span>
<p>J Gardner, G Pleiss, K Q Weinberger, D Bindel, and A G Wilson, “GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration,” <em>Advances in neural information processing systems</em>, vol. 31, 2018.</p>
</aside>
<aside class="footnote brackets" id="gramacy2020" role="note">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p>R B Gramacy, <em>Surrogates: Gaussian process modeling, design, and optimization for the applied sciences</em>, 1st ed. Boca Raton, FL: CRC press, 2020.</p>
</aside>
<aside class="footnote brackets" id="jones1998" role="note">
<span class="label"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id17">2</a>)</span>
<p>D R Jones, M Schonlau, and W J Welch, “Efficient global optimization of expensive black-box functions,” <em>Journal of global optimization</em>, vol. 13, no. 4, p. 566, 1998.</p>
</aside>
<aside class="footnote brackets" id="kingma2015" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">7</a><span class="fn-bracket">]</span></span>
<p>D P Kingma and J Ba, “Adam: A method for stochastic optimization,” <em>Proceedings of the 3rd international conference on learning representations</em>, 2015.</p>
</aside>
<aside class="footnote brackets" id="kraft1994" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">8</a><span class="fn-bracket">]</span></span>
<p>D Kraft, “Algorithm 733: TOMP-Fortran modules for optimal control calculations,” <em>ACM Transactions on Mathematical Software (TOMS)</em>, vol. 20, no. 3, p. 262-281, 1994.</p>
</aside>
<aside class="footnote brackets" id="mahfoze2019" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">9</a><span class="fn-bracket">]</span></span>
<p>O A Mahfoze, A Moody, A Wynn, R D Whalley, and S Laizet, “Reducing the skin-friction drag of a turbulent boundary-layer flow with low-amplitude wall-normal blowing within a Bayesian optimization framework,” <em>Physical review fluids</em>, vol. 4, no. 9, 2019.</p>
</aside>
<aside class="footnote brackets" id="mckay2000" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">10</a><span class="fn-bracket">]</span></span>
<p>M D McKay, R J Beckman, and W J Conover, “A comparison of three methods for selecting values of input variables in the analysis of output from a computer code,” <em>Technometrics</em>, vol. 42, no. 1, p. 55-61, 2000.</p>
</aside>
<aside class="footnote brackets" id="oconnor2023" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">11</a><span class="fn-bracket">]</span></span>
<p>J O’Connor, M Diessner, K Wilson, R D Whalley, A Wynn, and S Laizet, “Optimisation and analysis of streamwise-varying wall-normal blowing in a turbulent boundary layer,” <em>Flow, Turbulence and Combustion</em>, 2023.</p>
</aside>
<aside class="footnote brackets" id="shahriari2015" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">12</a><span class="fn-bracket">]</span></span>
<p>B Shahriari, K Swersky, Z Wang, R P Adams, and N De Freitas, “Taking the human out of the loop: A review of Bayesian optimization,” <em>Proceedings of the IEEE</em>, vol. 104, no. 1, p. 148-175, 2015.</p>
</aside>
<aside class="footnote brackets" id="snoek2012" role="note">
<span class="label"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id16">2</a>,<a role="doc-backlink" href="#id21">3</a>,<a role="doc-backlink" href="#id27">4</a>)</span>
<p>J Snoek, H Larochelle, and R P Adams, “Practical Bayesian optimization of machine learning algorithms,” <em>Advances in neural information processing systems</em>, vol. 25, 2012.</p>
</aside>
<aside class="footnote brackets" id="srinivas2010" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">14</a><span class="fn-bracket">]</span></span>
<p>N Srinivas, A Krause, S M Kakade, and M Seeger, “Gaussian process optimization in the bandit setting: No regret and experimental design,” <em>Proceedings of the 27th international conference on machine learning</em>, p. 1015-1022, 2010.</p>
</aside>
<aside class="footnote brackets" id="wang2022" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">15</a><span class="fn-bracket">]</span></span>
<p>K Wang and A W Dowling, “Bayesian optimization for chemical products and functional materials,” <em>Current opinion in chemical engineering</em>, vol. 36, 2022.</p>
</aside>
<aside class="footnote brackets" id="white2021" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">16</a><span class="fn-bracket">]</span></span>
<p>C White, W Neiswanger, and Y Savani, “Bananas: Bayesian optimization with neural architectures for neural architecture search,” <em>Proceedings of the AAAI conference on artificial intelligence</em>, vol. 35, no. 12, 2021.</p>
</aside>
<aside class="footnote brackets" id="williams2006" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">17</a><span class="fn-bracket">]</span></span>
<p>C K I Williams, and C E Rasmussen, <em>Gaussian processes for machine learning</em>, 2nd ed. Cambridge, MA: MIT press, 2006.</p>
</aside>
<aside class="footnote brackets" id="wilson2018" role="note">
<span class="label"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id22">1</a>,<a role="doc-backlink" href="#id25">2</a>,<a role="doc-backlink" href="#id26">3</a>)</span>
<p>J Wilson, F Hutter, and M Deisenroth, “Maximizing acquisition functions for Bayesian optimization,” <em>Advances in neural information processing systems</em>, vol. 31, 2018.</p>
</aside>
<aside class="footnote brackets" id="wu2019" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">19</a><span class="fn-bracket">]</span></span>
<p>J Wu, X Y Chen, H Zhang, L D Xiong, H Lei, and S H Deng, “Hyperparameter optimization for machine learning models based on Bayesian optimization,” <em>Journal of electronic science and technology</em>, vol. 17, no. 1, p. 26-40, 2019.</p>
</aside>
<aside class="footnote brackets" id="zhu1997" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">20</a><span class="fn-bracket">]</span></span>
<p>C Zhu, R H Byrd, P Lu, J Nocedal, “Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization,” <em>ACM Transactions on mathematical software (TOMS)</em>, vol. 23, no. 4, p. 550-560, 1997.</p>
</aside>
</aside>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="practical_considerations.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Practical considerations</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="get_started.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Get started</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Mike Diessner
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">A primer on Bayesian optimisation</a><ul>
<li><a class="reference internal" href="#maximisation-problem">Maximisation problem</a></li>
<li><a class="reference internal" href="#bayesian-optimisation">Bayesian optimisation</a></li>
<li><a class="reference internal" href="#surrogate-model">Surrogate model</a></li>
<li><a class="reference internal" href="#acquisition-function">Acquisition function</a><ul>
<li><a class="reference internal" href="#analytical-acquisition-functions">Analytical acquisition functions</a></li>
<li><a class="reference internal" href="#monte-carlo-acquisition-functions">Monte Carlo acquisition functions</a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>