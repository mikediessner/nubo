{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu1CcMlRlhtO"
      },
      "source": [
        "# Bayesian optimisation with known observational noise\n",
        "\n",
        "In this example, NUBO is used for sequential single-point optimisation for situations where the observational noise from taking the measurements is known. We assume that the variance of the observational noise of our black box funcion (simulated here by the `Hartmann6D` function) to be equal to $\\sigma^2 = 0.025$. The Bayesian optimisation loop compared to the case with unknown noise differs only in terms of the likelihood that we use. Here, we use the `FixedNoiseGaussianLikelihood` and specify the observational noise variance for each data point (for this example, the same variance is used for all points). We also allow the likelihood to estimate any additional noise. The optimisation loop is run for 40 iterations and finds a solution close the true optimum of -3.3224."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ivtHr7RbldhU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best at evaluation 35: \t Inputs: [0.1645 0.2206 0.3035 0.2072 0.194  0.869 ], \t Outputs: [-1.6695]\n",
            "New best at evaluation 36: \t Inputs: [0.4713 0.7459 1.     0.6095 0.5726 0.    ], \t Outputs: [-2.3638]\n",
            "New best at evaluation 42: \t Inputs: [0.1779 0.0974 0.3987 0.2433 0.2859 0.7401], \t Outputs: [-2.9697]\n",
            "New best at evaluation 44: \t Inputs: [0.414  0.8375 1.     0.5714 0.5749 0.    ], \t Outputs: [-2.9842]\n",
            "New best at evaluation 45: \t Inputs: [0.2113 0.0234 0.4707 0.2358 0.286  0.7096], \t Outputs: [-3.0098]\n",
            "New best at evaluation 52: \t Inputs: [0.3824 0.8989 0.7951 0.5489 0.5283 0.    ], \t Outputs: [-3.0416]\n",
            "New best at evaluation 57: \t Inputs: [0.2381 0.1042 0.4973 0.2776 0.2888 0.6892], \t Outputs: [-3.2606]\n",
            "New best at evaluation 59: \t Inputs: [0.2256 0.0998 0.4559 0.2737 0.2983 0.6449], \t Outputs: [-3.297]\n",
            "Evaluation: 59 \t Solution: -3.2970\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from nubo.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
        "from nubo.models import GaussianProcess, fit_gp\n",
        "from nubo.optimisation import lbfgsb\n",
        "from nubo.test_functions import Hartmann6D\n",
        "from nubo.utils import gen_inputs\n",
        "from gpytorch.likelihoods import FixedNoiseGaussianLikelihood\n",
        "\n",
        "\n",
        "# test function\n",
        "func = Hartmann6D(minimise=False, noise_std=0.025)\n",
        "dims = func.dims\n",
        "bounds = func.bounds\n",
        "\n",
        "# training data\n",
        "x_train = gen_inputs(num_points=dims*5,\n",
        "                     num_dims=dims,\n",
        "                     bounds=bounds)\n",
        "y_train = func(x_train)\n",
        "\n",
        "# Bayesian optimisation loop\n",
        "iters = 40\n",
        "\n",
        "for iter in range(iters):\n",
        "    \n",
        "    # specify Gaussian process\n",
        "    likelihood = FixedNoiseGaussianLikelihood(noise=torch.ones(x_train.size(0))*0.025, learn_additional_noise=True)\n",
        "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
        "    \n",
        "    # fit Gaussian process\n",
        "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
        "\n",
        "    # specify acquisition function\n",
        "    # acq = ExpectedImprovement(gp=gp, y_best=torch.max(y_train))\n",
        "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
        "\n",
        "    # optimise acquisition function\n",
        "    x_new, _ = lbfgsb(func=acq, bounds=bounds, num_starts=5)\n",
        "\n",
        "    # evaluate new point\n",
        "    y_new = func(x_new)\n",
        "    \n",
        "    # add to data\n",
        "    x_train = torch.vstack((x_train, x_new))\n",
        "    y_train = torch.hstack((y_train, y_new))\n",
        "\n",
        "    # print new best\n",
        "    if y_new > torch.max(y_train[:-1]):\n",
        "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
        "\n",
        "# results\n",
        "best_iter = int(torch.argmax(y_train))\n",
        "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
