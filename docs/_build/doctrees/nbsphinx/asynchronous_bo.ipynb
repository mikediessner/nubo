{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Bayesian optimisation\n",
    "\n",
    "In this example, NUBO is used for asynchronous optimisation. This means that the optimisation loop is continued while some points are still being evaluated from the objective function. This is particularly useful for situations in which some evaluations take a longer time to complete but you do not want to waste time by waiting for these pending observations. In the script below, we randomly sample a pending point `x_pending` and assume that we are still waiting for its output. While waiting, we continue the optimisation loop for 10 iterations with a batch size of 4 each (a total of 40 evaluations) and finds a solution close the true optimum of -3.3224. The `Hartmann6D` synthetic test function acts as a surrogate for a black box objective funtion, such as an experiment or a simulation. Notice that we provide the pending point `x_pending` to the acquisition function `MCUpperConfidenceBound` as an argument. For asynchronous optimisation, Monte Carlo acquisition functions have to be used as this process is in general intractable for analytical functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point pending evaluation: [0.1874 0.0246 0.4963 0.5481 0.5602 0.5434]\n",
      "New best at evaluation 35: \t Inputs: [0.4185 0.9953 0.9987 0.4276 0.5028 0.0036], \t Outputs: -2.2505\n",
      "New best at evaluation 48: \t Inputs: [0.403  0.9975 0.0053 0.4634 0.0037 0.0044], \t Outputs: -2.4121\n",
      "New best at evaluation 53: \t Inputs: [4.010e-01 8.998e-01 9.766e-01 4.932e-01 1.000e-04 5.000e-04], \t Outputs: -2.926\n",
      "New best at evaluation 56: \t Inputs: [4.135e-01 8.917e-01 9.949e-01 5.736e-01 7.400e-03 2.000e-04], \t Outputs: -3.1253\n",
      "New best at evaluation 61: \t Inputs: [0.4098 0.8883 0.9959 0.5746 0.01   0.0463], \t Outputs: -3.1903\n",
      "Evaluation: 61 \t Solution: -3.1903\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.acquisition import MCExpectedImprovement, MCUpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import multi_joint\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = func.dims\n",
    "bounds = func.bounds\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# point pending evaluation\n",
    "x_pending = torch.rand((1, dims))\n",
    "print(f\"Point pending evaluation: {x_pending.numpy().reshape(dims).round(4)}\")\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 10\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = MCExpectedImprovement(gp=gp, y_best=torch.max(y_train), x_pending=x_pending, samples=256)\n",
    "    acq = MCUpperConfidenceBound(gp=gp, beta=1.96**2, x_pending=x_pending, samples=256)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = multi_joint(func=acq, method=\"Adam\", batch_size=4, bounds=bounds, lr=0.1, steps=200, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if torch.max(y_new) > torch.max(y_train[:-y_new.size(0)]):\n",
    "        best_eval = torch.argmax(y_train)\n",
    "        print(f\"New best at evaluation {best_eval+1}: \\t Inputs: {x_train[best_eval, :].numpy().reshape(dims).round(4)}, \\t Outputs: {-y_train[best_eval].numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
