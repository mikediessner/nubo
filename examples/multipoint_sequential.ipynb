{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel multi-point sequential Bayesian optimisation\n",
    "\n",
    "In this example, NUBO is used to perform multi-point optimisation that allows the candidates to be evaluated from the objective function in parallel. Multi-point optimisation is implemented in NUBO through Monte Carlo acquisition functions. The script below uses the `MCUpperConfidenceBound` acquisition function with 512 samples and resamples the base samples (default). Each batch of 4 is found sequentialy with the `multi_sequential()` function by optimising the acquisition function with the stochastic Adam optimiser. We could also fix the base samples in `MCUpperConfidenceBound` and use a deterministic optimiser, such as L-BFGS-B or SLSQP. The `Hartmann6D` synthetic test function acts as a surrogate for a black box objective funtion, such as an experiment or a simulation. The optimisation loop is run for 10 iterations returning batches of 4 each (a total of 40 evaluations) and finds a solution close the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 31: \t Inputs: [0.3487 0.7475 0.8996 0.5506 0.5202 0.1044], \t Outputs: -2.4199\n",
      "New best at evaluation 43: \t Inputs: [0.3364 0.7636 0.8507 0.5472 0.5597 0.0467], \t Outputs: -2.5693\n",
      "New best at evaluation 51: \t Inputs: [0.3677 0.8093 0.968  0.5548 0.3805 0.0176], \t Outputs: -2.9473\n",
      "New best at evaluation 55: \t Inputs: [3.770e-01 8.706e-01 9.928e-01 5.756e-01 8.700e-03 8.000e-04], \t Outputs: -3.0891\n",
      "New best at evaluation 63: \t Inputs: [0.4021 0.8817 0.9896 0.5661 0.001  0.0443], \t Outputs: -3.1908\n",
      "New best at evaluation 67: \t Inputs: [0.4077 0.8731 0.9979 0.5741 0.0034 0.0394], \t Outputs: -3.1919\n",
      "Evaluation: 67 \t Solution: -3.1919\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.acquisition import MCExpectedImprovement, MCUpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import multi_sequential\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 10\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = MCExpectedImprovement(gp=gp, y_best=torch.max(y_train), samples=512)\n",
    "    acq = MCUpperConfidenceBound(gp=gp, beta=1.96**2, samples=512)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = multi_sequential(func=acq, method=\"Adam\", batch_size=4, bounds=bounds, lr=0.1, steps=200, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if torch.max(y_new) > torch.max(y_train[:-y_new.size(0)]):\n",
    "        best_eval = torch.argmax(y_train)\n",
    "        print(f\"New best at evaluation {best_eval+1}: \\t Inputs: {x_train[best_eval, :].numpy().reshape(dims).round(4)}, \\t Outputs: {-y_train[best_eval].numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
