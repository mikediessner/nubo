{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started\n",
    "\n",
    "## Installing NUBO\n",
    "\n",
    "Install NUBO and all its dependencies directly from the GitHub repository using `pip` with the following code. The use of a virtual environment is recommended.\n",
    "\n",
    "```text\n",
    "pip install git+https://github.com/mikediessner/nubo\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "## Optimising a toy function with NUBO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set-up the toy function we want to optimise. In this case we choose the 6-dimensional Hartmann function, a multi-modal function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nubo.test_functions import Hartmann6D\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = func.dims\n",
    "bounds = func.bounds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate some initial training data. We decide to generate 5 data points per input dimension resulting in a total of 30 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nubo.utils import gen_inputs\n",
    "\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can prepare the Bayesian optimisation loop. We choose a in NUBO pre-defined Gaussian process with constant mean function and Matern 5/2 kernel and estimate its hyper-parameters via maximum likelihood estimation (MLE) using the Adam optimiser. For the acquisition function, we implement the analytical upper confidence bound (UCB) with trade-off parameter $\\beta=1.96^2$ (corresponding to 95% confidence intervals for a Gaussian distribution) and optimise it with the L-BFGS-B algorithm using a multi-start approach with five restarts. The Bayesian optimisation loop is run for 40 iterations giving an evaluation budget of 70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 43: \t Inputs: [0.3949 1.     1.     0.7699 0.0393 0.0369], \t Outputs: [-1.9498]\n",
      "New best at evaluation 52: \t Inputs: [0.2581 0.3436 0.5644 0.2322 0.3715 0.8276], \t Outputs: [-2.1738]\n",
      "New best at evaluation 56: \t Inputs: [0.4257 1.     1.     0.6889 0.094  0.003 ], \t Outputs: [-2.4506]\n",
      "New best at evaluation 59: \t Inputs: [0.2707 0.2744 0.5454 0.2384 0.3474 0.7427], \t Outputs: [-2.8153]\n",
      "New best at evaluation 60: \t Inputs: [0.3071 0.2052 0.4839 0.265  0.3319 0.6998], \t Outputs: [-3.091]\n",
      "New best at evaluation 69: \t Inputs: [0.2485 0.1512 0.4608 0.291  0.3224 0.6786], \t Outputs: [-3.2718]\n"
     ]
    }
   ],
   "source": [
    "from nubo.acquisition import UpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import lbfgsb\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = lbfgsb(func=acq, bounds=bounds, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print the overall best solution: We get -3.2718 on evaluation 69 which approximaties the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: 69 \t Solution: 3.2718\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {float(y_train[best_iter]):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated parameters of the Gaussian process can be viewed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean function constant: 0.18545642495155334\n",
      "Covariance kernel output-scale: 0.36589178442955017\n",
      "Covariance kernel length-scale: tensor([[0.3780, 0.4826, 0.6710, 0.3035, 0.3445, 0.3133]])\n",
      "Estimated noise / nugget: 0.0008741075871512294\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean function constant: {gp.mean_module.constant.item()}\")\n",
    "print(f\"Covariance kernel output-scale: {gp.covar_module.outputscale.item()}\")\n",
    "print(f\"Covariance kernel length-scale: {gp.covar_module.base_kernel.lengthscale.detach()}\")\n",
    "print(f\"Estimated noise/nugget: {likelihood.noise.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
