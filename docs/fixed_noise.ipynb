{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu1CcMlRlhtO"
      },
      "source": [
        "# Bayesian optimisation with known observational noise\n",
        "\n",
        "In this example, NUBO is used for sequential single-point optimisation for situations where the observational noise from taking the measurements is known. We assume that the variance of the observational noise of our black box funcion (simulated here by the `Hartmann6D` function) to be equal to $\\sigma^2 = 0.025$. The Bayesian optimisation loop compared to the case with unknown noise differs only in terms of the likelihood that we use. Here, we use the `FixedNoiseGaussianLikelihood` and specify the observational noise variance for each data point (for this example, the same variance is used for all points). We also allow the likelihood to estimate any additional noise. The optimisation loop is run for 40 iterations and finds a solution close the true optimum of -3.3224."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ivtHr7RbldhU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best at evaluation 31: \t Inputs: [0.3889 1.     0.5357 0.5685 0.8369 0.    ], \t Outputs: [-2.6202]\n",
            "New best at evaluation 33: \t Inputs: [0.391  1.     0.5254 0.5547 0.8366 0.    ], \t Outputs: [-2.6486]\n",
            "New best at evaluation 39: \t Inputs: [0.3754 1.     1.     0.5115 0.     0.    ], \t Outputs: [-2.6733]\n",
            "New best at evaluation 41: \t Inputs: [0.3804 1.     1.     0.5882 0.     0.    ], \t Outputs: [-2.7625]\n",
            "New best at evaluation 42: \t Inputs: [0.3894 0.9043 1.     0.5801 0.     0.    ], \t Outputs: [-3.085]\n",
            "New best at evaluation 47: \t Inputs: [0.4179 0.8636 1.     0.5875 0.     0.    ], \t Outputs: [-3.0931]\n",
            "New best at evaluation 51: \t Inputs: [0.4216 0.8661 1.     0.5709 0.     0.    ], \t Outputs: [-3.1223]\n",
            "New best at evaluation 53: \t Inputs: [0.4138 0.8594 1.     0.5552 0.     0.    ], \t Outputs: [-3.1599]\n",
            "New best at evaluation 60: \t Inputs: [0.4135 0.8788 1.     0.5735 0.     0.039 ], \t Outputs: [-3.2312]\n",
            "Evaluation: 60 \t Solution: -3.2312\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from nubo.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
        "from nubo.models import GaussianProcess, fit_gp\n",
        "from nubo.optimisation import single\n",
        "from nubo.test_functions import Hartmann6D\n",
        "from nubo.utils import gen_inputs\n",
        "from gpytorch.likelihoods import FixedNoiseGaussianLikelihood\n",
        "\n",
        "\n",
        "# test function\n",
        "func = Hartmann6D(minimise=False, noise_std=0.025)\n",
        "dims = func.dims\n",
        "bounds = func.bounds\n",
        "\n",
        "# training data\n",
        "x_train = gen_inputs(num_points=dims*5,\n",
        "                     num_dims=dims,\n",
        "                     bounds=bounds)\n",
        "y_train = func(x_train)\n",
        "\n",
        "# Bayesian optimisation loop\n",
        "iters = 40\n",
        "\n",
        "for iter in range(iters):\n",
        "    \n",
        "    # specify Gaussian process\n",
        "    likelihood = FixedNoiseGaussianLikelihood(noise=torch.ones(x_train.size(0))*0.025, learn_additional_noise=True)\n",
        "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
        "    \n",
        "    # fit Gaussian process\n",
        "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
        "\n",
        "    # specify acquisition function\n",
        "    # acq = ExpectedImprovement(gp=gp, y_best=torch.max(y_train))\n",
        "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
        "\n",
        "    # optimise acquisition function\n",
        "    x_new, _ = single(func=acq, method=\"L-BFGS-B\", bounds=bounds, num_starts=5)\n",
        "\n",
        "    # evaluate new point\n",
        "    y_new = func(x_new)\n",
        "    \n",
        "    # add to data\n",
        "    x_train = torch.vstack((x_train, x_new))\n",
        "    y_train = torch.hstack((y_train, y_new))\n",
        "\n",
        "    # print new best\n",
        "    if y_new > torch.max(y_train[:-1]):\n",
        "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
        "\n",
        "# results\n",
        "best_iter = int(torch.argmax(y_train))\n",
        "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "440ccce7314e8ea21bc6387ad3e4b0d06ade5f0dbc76119080186fc1c6dec90d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
