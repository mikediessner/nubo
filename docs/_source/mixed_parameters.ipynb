{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimisation with continuous and discrete parameters\n",
    "\n",
    "In this example, NUBO is used for sequential single-point optimisation with continuous and discrete parameters. Additionally to the bounds, a dictionary containing the dimensions as keys and the possible values as values has to be specified. The `Hartmann6D` synthetic test function acts as a substitute for a black box objective funtion, such as an experiment or a simulation. We use the analytical acquisiton function `UpperConfidenceBound` with $\\beta = 1.96^2$ corresponding to the 95% confidence interval of the Gaussian distribution. We optimise this acquisition function with the L-BFGS-B algorithm with 5 starts to avoid getting stuck in a local maximum. The optimisation loop is run for 40 iterations and finds a solution close the true optimum of -3.3224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best at evaluation 41: \t Inputs: [0.4   0.328 1.    0.323 0.3   1.   ], \t Outputs: [-0.984]\n",
      "New best at evaluation 42: \t Inputs: [0.2    0.3743 1.     0.3532 0.3    0.9297], \t Outputs: [-1.165]\n",
      "New best at evaluation 43: \t Inputs: [0.2    0.3774 1.     0.3439 0.3    0.8455], \t Outputs: [-1.2931]\n",
      "New best at evaluation 44: \t Inputs: [0.2    0.317  1.     0.3622 0.3    0.8081], \t Outputs: [-1.3629]\n",
      "New best at evaluation 46: \t Inputs: [0.2    0.3075 1.     0.3253 0.3    0.7696], \t Outputs: [-1.4464]\n",
      "New best at evaluation 47: \t Inputs: [0.2    0.274  1.     0.2838 0.3    0.7108], \t Outputs: [-1.5098]\n",
      "New best at evaluation 48: \t Inputs: [0.2    0.2371 0.7806 0.2867 0.3    0.7069], \t Outputs: [-2.4508]\n",
      "New best at evaluation 49: \t Inputs: [0.2    0.1603 0.7432 0.2521 0.3    0.7525], \t Outputs: [-2.526]\n",
      "New best at evaluation 50: \t Inputs: [0.2    0.2488 0.6924 0.2493 0.3    0.766 ], \t Outputs: [-2.642]\n",
      "New best at evaluation 51: \t Inputs: [0.2    0.1938 0.6262 0.2829 0.3    0.7344], \t Outputs: [-2.9647]\n",
      "New best at evaluation 52: \t Inputs: [0.2    0.1336 0.5351 0.283  0.3    0.5795], \t Outputs: [-3.1219]\n",
      "New best at evaluation 54: \t Inputs: [0.2    0.2051 0.4751 0.3168 0.3    0.6334], \t Outputs: [-3.2154]\n",
      "New best at evaluation 56: \t Inputs: [0.2    0.1743 0.4045 0.2843 0.3    0.6907], \t Outputs: [-3.239]\n",
      "New best at evaluation 60: \t Inputs: [0.2    0.1582 0.4587 0.2535 0.3    0.6505], \t Outputs: [-3.2954]\n",
      "New best at evaluation 61: \t Inputs: [0.2    0.1474 0.4688 0.2743 0.3    0.6572], \t Outputs: [-3.315]\n",
      "New best at evaluation 62: \t Inputs: [0.2    0.1487 0.4694 0.2756 0.3    0.6573], \t Outputs: [-3.3152]\n",
      "Evaluation: 62 \t Solution: -3.3152\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nubo.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
    "from nubo.models import GaussianProcess, fit_gp\n",
    "from nubo.optimisation import single\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import gen_inputs\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = 6\n",
    "\n",
    "# specify bounds and discrete values\n",
    "bounds = torch.tensor([[0., 0., 0., 0., 0., 0.], [1., 1., 1., 1., 1., 1.]])\n",
    "discrete = {0: [0.2, 0.4, 0.6, 0.8], 4: [0.3, 0.6, 0.9]}\n",
    "\n",
    "# training data\n",
    "x_train = gen_inputs(num_points=dims*5,\n",
    "                     num_dims=dims,\n",
    "                     bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood()\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    \n",
    "    # fit Gaussian process\n",
    "    fit_gp(x_train, y_train, gp=gp, likelihood=likelihood, lr=0.1, steps=200)\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = ExpectedImprovement(gp=gp, y_best=torch.max(y_train))\n",
    "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = single(func=acq, method=\"L-BFGS-B\", bounds=bounds, discrete=discrete, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims).round(4)}, \\t Outputs: {-y_new.numpy().round(4)}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {-float(y_train[best_iter]):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
