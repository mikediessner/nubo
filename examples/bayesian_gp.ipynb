{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Bayesian Gaussian Process for Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nubo.acquisition import ExpectedImprovement, UpperConfidenceBound\n",
    "from nubo.models import GaussianProcess\n",
    "from nubo.optimisation import lbfgsb\n",
    "from nubo.test_functions import Hartmann6D\n",
    "from nubo.utils import LatinHypercubeSampling, unnormalise\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import pyro\n",
    "from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.priors import LogNormalPrior, NormalPrior, UniformPrior\n",
    "from gpytorch.settings import fast_computations\n",
    "\n",
    "\n",
    "# test function\n",
    "func = Hartmann6D(minimise=False)\n",
    "dims = func.dims\n",
    "bounds = func.bounds\n",
    "\n",
    "# training data\n",
    "torch.manual_seed(1)\n",
    "lhs = LatinHypercubeSampling(dims=dims)\n",
    "x_train = lhs.maximin(points=dims*5)\n",
    "x_train = unnormalise(x_train, bounds=bounds)\n",
    "y_train = func(x_train)\n",
    "\n",
    "# Bayesian optimisation loop\n",
    "iters = 40\n",
    "\n",
    "for iter in range(iters):\n",
    "    \n",
    "    # specify Gaussian process\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Positive())\n",
    "    gp = GaussianProcess(x_train, y_train, likelihood=likelihood)\n",
    "    gp.mean_module.register_prior(\"mean_prior\", UniformPrior(-1, 1), \"constant\")\n",
    "    gp.covar_module.base_kernel.register_prior(\"lengthscale_prior\", UniformPrior(0.01, 0.5), \"lengthscale\")\n",
    "    gp.covar_module.register_prior(\"outputscale_prior\", UniformPrior(1, 2), \"outputscale\")\n",
    "    likelihood.register_prior(\"noise_prior\", UniformPrior(0.01, 0.5), \"noise\")\n",
    "    mll = ExactMarginalLogLikelihood(likelihood, gp)\n",
    "\n",
    "    # set up pyro model for sampling\n",
    "    def pyro_gp(x, y):\n",
    "        with fast_computations(False, False, False):\n",
    "            sampled_gp = gp.pyro_sample_from_prior()\n",
    "            output = sampled_gp.likelihood(sampled_gp(x))\n",
    "            pyro.sample(\"obs\", output, obs=y)\n",
    "        return y\n",
    "\n",
    "    # run MCMC\n",
    "    nuts_kernel = NUTS(pyro_gp)\n",
    "    mcmc_run = MCMC(nuts_kernel, num_samples=128, warmup_steps=128, disable_progbar=True)\n",
    "    mcmc_run.run(x_train, y_train)\n",
    "\n",
    "    # load MCMC samples into model\n",
    "    gp.pyro_load_from_samples(mcmc_run.get_samples())\n",
    "\n",
    "    # specify acquisition function\n",
    "    # acq = ExpectedImprovement(gp=gp, y_best=torch.max(y_train))\n",
    "    acq = UpperConfidenceBound(gp=gp, beta=1.96**2)\n",
    "\n",
    "    # optimise acquisition function\n",
    "    x_new, _ = lbfgsb(func=lambda x: sum(acq(x))/mcmc_run.num_samples, bounds=bounds, num_starts=5)\n",
    "\n",
    "    # evaluate new point\n",
    "    y_new = func(x_new)\n",
    "    \n",
    "    # add to data\n",
    "    x_train = torch.vstack((x_train, x_new))\n",
    "    y_train = torch.hstack((y_train, y_new))\n",
    "\n",
    "    # print new best\n",
    "    if y_new > torch.max(y_train[:-1]):\n",
    "        print(f\"New best at evaluation {len(y_train)}: \\t Inputs: {x_new.numpy().reshape(dims)}, \\t Outputs: {-y_new.numpy()}\")\n",
    "\n",
    "# results\n",
    "best_iter = int(torch.argmax(y_train))\n",
    "print(f\"Evaluation: {best_iter+1} \\t Solution: {float(y_train[best_iter]):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
